{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import scipy.ndimage as ndimage\n",
    "from torchvision import transforms, utils\n",
    "from toolz.curried import pipe, curry, compose\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "import scores\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "\n",
    "print(names)\n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "print(df_train.shape)\n",
    "print(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "\n",
    "df_stable=df_train.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "print(df_stable.shape)\n",
    "df_stable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_new=df_stable[feature_cols] #training only on stable elements\n",
    "#y_target=df_stable[stab_vec_list]\n",
    "###\n",
    "\n",
    "X_train_new=df_train[feature_cols]   #training  on all elements\n",
    "y_target=df_train[stab_vec_list[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "X_train_new_mag_1=pd.DataFrame(data=X_train_new_mag_1,columns=feature_cols)\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "X_train_new_0_1=pd.DataFrame(data=X_train_new_0_1,columns=feature_cols)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "X_train_new_m1_p1=pd.DataFrame(data=X_train_new_m1_p1,columns=feature_cols)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "\n",
    "## Taking Logarithm of High Values\n",
    "\n",
    "X_train_new_log=X_train_new.copy()\n",
    "X_train_new_log[X_train_new_log>100]=X_train_new_log[X_train_new_log>100].apply(np.log)\n",
    "print(X_train_new_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument_group('Optimization related arguments')\n",
    "parser.add_argument('-num_epochs', default=100, type=int, help='Epochs')\n",
    "parser.add_argument('-batch_size', default=20, type=int, help='Batch size')\n",
    "parser.add_argument('-lr', default=1e-3, type=float, help='Learning rate')\n",
    "parser.add_argument('-lr_decay_rate', default=0.9997592083, type=float, help='Decay for lr')\n",
    "parser.add_argument('-min_lr', default=5e-5, type=float, help='Minimum learning rate')\n",
    "parser.add_argument('-weight_init', default='xavier', choices=['xavier', 'kaiming'], help='Weight initialization strategy')\n",
    "parser.add_argument('-overfit', action='store_true', help='Overfit on 5 examples, meant for debugging')\n",
    "parser.add_argument('-gpuid', default=-1, type=int, help='GPU id to use')\n",
    "        \n",
    "parser.add_argument('-test_size', default=0.15)\n",
    "\n",
    "parser.add_argument_group('Checkpointing related arguments')\n",
    "parser.add_argument('-load_path', default='', help='Checkpoint to load path from')\n",
    "parser.add_argument('-save_path', default='checkpoints/', help='Path to save checkpoints')\n",
    "parser.add_argument('-save_step', default=2, type=int, help='Save checkpoint after every save_step epochs')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# input arguments and options\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.strftime(datetime.datetime.utcnow(), '%d-%b-%Y-%H:%M:%S')\n",
    "if args.save_path == 'checkpoints/':\n",
    "    args.save_path += start_time\n",
    "    \n",
    "if args.load_path != '':\n",
    "    components = torch.load(args.load_path)\n",
    "    model_args = components['model_args']\n",
    "    model_args.gpuid = args.gpuid\n",
    "    model_args.batch_size = args.batch_size\n",
    "\n",
    "    # this is required by dataloader\n",
    "    args.normalize = model_args.normalize\n",
    "\n",
    "for arg in vars(args):\n",
    "    print('{:<20}: {}'.format(arg, getattr(args, arg)))\n",
    "    \n",
    "# transfer all options to model\n",
    "model_args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_dataset(Dataset):\n",
    "    \"\"\"Face landmarks Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data, y_true,args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"        \n",
    "        self.data=data\n",
    "        self.y_true=y_true\n",
    "        self.args = args\n",
    "        assert len(data) == len(y_true)\n",
    "        \n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, y_true,\n",
    "                                                            test_size=args.test_size,\n",
    "                                                            shuffle=True,\n",
    "                                                            random_state=42)\n",
    "        self.data = data\n",
    "        self.y_true = y_true\n",
    "\n",
    "        self.X_train = torch.from_numpy(X_train.values).float()\n",
    "        self.y_train = torch.from_numpy(y_train.values).float()\n",
    "        self.X_test = torch.from_numpy(X_test.values).float()\n",
    "        self.y_test = torch.from_numpy(y_test.values).float()\n",
    "\n",
    "        self.num_data_points = {}\n",
    "        self.num_data_points['train'] = len(X_train)\n",
    "        self.num_data_points['test'] = len(X_test)\n",
    "        \n",
    "        self._split = 'train'\n",
    "\n",
    "    @property\n",
    "    def split(self):\n",
    "        return self._split\n",
    "\n",
    "    @split.setter\n",
    "    def split(self, split):\n",
    "        self._split = split\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # methods to override - __len__ and __getitem__ methods\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data_points[self._split]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dtype = self._split\n",
    "        item = {'index': idx}\n",
    "        item['features'] = self.X_train[idx,:]\n",
    "        item['outputs'] = self.y_train[idx]\n",
    "        return item\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "    # collate function utilized by dataloader for batching\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        dtype = self._split\n",
    "        merged_batch = {key: [d[key] for d in batch] for key in batch[0]}\n",
    "        out = {}\n",
    "        for key in merged_batch:\n",
    "            if key in {'index'}:\n",
    "                out[key] = merged_batch[key]\n",
    "            else:\n",
    "                out[key] = torch.stack(merged_batch[key], 0)\n",
    "\n",
    "        batch_keys = ['features', 'outputs']\n",
    "        return {key: out[key] for key in batch_keys}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(98, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(60, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(30, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(15, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, batch):\n",
    "        return self.layers(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = arg_dataset(X_train_new_m1_p1, y_target,args)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = dataset[0]\n",
    "\n",
    "x = item['features'][None]\n",
    "print(\"features: \", x)\n",
    "y = item['outputs']\n",
    "print(\"outputs: \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(args, 'iter_per_epoch', math.ceil(dataset.num_data_points['train'] / args.batch_size))\n",
    "print(\"{} iter per epoch.\".format(args.iter_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(model_args)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.lr_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# training\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "net.train()\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "\n",
    "running_loss = 0.0\n",
    "train_begin = datetime.datetime.utcnow()\n",
    "print(\"Training start time: {}\".format(datetime.datetime.strftime(train_begin, '%d-%b-%Y-%H:%M:%S')))\n",
    "\n",
    "log_loss = []\n",
    "for epoch in range(1, model_args.num_epochs + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        for key in batch:\n",
    "            batch[key] = Variable(batch[key])\n",
    "            if args.gpuid >= 0:\n",
    "                batch[key] = batch[key].cuda()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # forward-backward pass and optimizer step\n",
    "        # --------------------------------------------------------------------\n",
    "        net_out = net(batch['features'])\n",
    "\n",
    "        cur_loss = criterion(net_out, batch['outputs'])\n",
    "        cur_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        gc.collect()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # update running loss and decay learning rates\n",
    "        # --------------------------------------------------------------------\n",
    "        #train_loss = cur_loss.data[0]\n",
    "        train_loss = cur_loss.item()\n",
    "        if running_loss > 0.0:\n",
    "            running_loss = 0.95 * running_loss + 0.05 * train_loss\n",
    "        else:\n",
    "            running_loss = train_loss \n",
    "\n",
    "        if optimizer.param_groups[0]['lr'] > args.min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # print after every few iterations\n",
    "        # --------------------------------------------------------------------\n",
    "        if i % 40 == 0:\n",
    "            test_losses = []\n",
    "            accuracy = []\n",
    "            for i in range(int(dataset.num_data_points['test']/args.batch_size)):\n",
    "                test_feat = dataset.X_test[i*args.batch_size:(i+1)*args.batch_size, :]\n",
    "                test_labels = dataset.y_test[i*args.batch_size:(i+1)*args.batch_size]\n",
    "                test_feat = Variable(test_feat)\n",
    "                test_labels = Variable(test_labels)\n",
    "                if args.gpuid >= 0:\n",
    "                    test_feat = test_feat.cuda()\n",
    "                    test_labels = test_labels.cuda()\n",
    "                net_out = net(test_feat)\n",
    "                cur_loss = criterion(net_out, test_labels)\n",
    "                test_losses.append(cur_loss.item())\n",
    "                \n",
    "                y_pred = torch.sigmoid(net_out).data > 0.5\n",
    "                y_pred = y_pred.cpu().numpy()\n",
    "                accuracy.append((test_labels.cpu().numpy() == y_pred).all(axis=1))\n",
    "\n",
    "            validation_loss = np.mean(test_losses)\n",
    "            \n",
    "            accuracy = np.mean(accuracy)\n",
    "\n",
    "            iteration = (epoch - 1) * args.iter_per_epoch + i\n",
    "\n",
    "            log_loss.append((epoch,\n",
    "                             iteration,\n",
    "                             running_loss,\n",
    "                             train_loss,\n",
    "                             validation_loss,\n",
    "                             accuracy,\n",
    "                             optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            # print current time, running average, learning rate, iteration, epoch\n",
    "            print(\"[{}][Epoch: {:3d}][Iter: {:6d}][Loss: {:6f}][val loss: {:6f}][accuracy: {:6f}][lr: {:7f}]\".format(\n",
    "                datetime.datetime.utcnow() - train_begin, epoch,\n",
    "                    iteration, running_loss, validation_loss, accuracy,\n",
    "                    optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # save checkpoints and final model\n",
    "    # ------------------------------------------------------------------------\n",
    "    if epoch % args.save_step == 0:\n",
    "        torch.save({\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': net.args\n",
    "        }, os.path.join(args.save_path, 'model_epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "torch.save({\n",
    "    'net': net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_args': net.args\n",
    "}, os.path.join(args.save_path, 'model_final.pth'))\n",
    "\n",
    "np.save(os.path.join(args.save_path, 'log_loss'), log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
