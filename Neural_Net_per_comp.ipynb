{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import scipy.ndimage as ndimage\n",
    "from torchvision import transforms, utils\n",
    "from toolz.curried import pipe, curry, compose\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "import scores\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_data.csv', 'test_data.csv']\n",
      "(2572, 99)\n",
      "/Users/davidmontesdeoca/Desktop/challenge_data/data/training_data.csv\n"
     ]
    }
   ],
   "source": [
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "\n",
    "print(names)\n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "print(df_train.shape)\n",
    "print(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2572, 110)\n",
      "(2572, 110)\n",
      "(1228, 110)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formulaA</th>\n",
       "      <th>formulaB</th>\n",
       "      <th>formulaA_elements_AtomicVolume</th>\n",
       "      <th>formulaB_elements_AtomicVolume</th>\n",
       "      <th>formulaA_elements_AtomicWeight</th>\n",
       "      <th>formulaB_elements_AtomicWeight</th>\n",
       "      <th>formulaA_elements_BoilingT</th>\n",
       "      <th>formulaB_elements_BoilingT</th>\n",
       "      <th>formulaA_elements_BulkModulus</th>\n",
       "      <th>formulaB_elements_BulkModulus</th>\n",
       "      <th>...</th>\n",
       "      <th>A82B</th>\n",
       "      <th>A73B</th>\n",
       "      <th>A64B</th>\n",
       "      <th>A55B</th>\n",
       "      <th>A46B</th>\n",
       "      <th>A37B</th>\n",
       "      <th>A28B</th>\n",
       "      <th>A19B</th>\n",
       "      <th>B</th>\n",
       "      <th>Stable_compunds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>47</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>17.075648</td>\n",
       "      <td>227.0</td>\n",
       "      <td>107.868200</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>16.594425</td>\n",
       "      <td>227.0</td>\n",
       "      <td>26.981539</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>89</td>\n",
       "      <td>35</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>42.527825</td>\n",
       "      <td>227.0</td>\n",
       "      <td>79.904000</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>89</td>\n",
       "      <td>49</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>26.082658</td>\n",
       "      <td>227.0</td>\n",
       "      <td>114.818000</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2345.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>89</td>\n",
       "      <td>81</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>28.640877</td>\n",
       "      <td>227.0</td>\n",
       "      <td>204.383300</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>1746.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    formulaA  formulaB  formulaA_elements_AtomicVolume  \\\n",
       "0         89        47                       37.433086   \n",
       "1         89        13                       37.433086   \n",
       "5         89        35                       37.433086   \n",
       "10        89        49                       37.433086   \n",
       "28        89        81                       37.433086   \n",
       "\n",
       "    formulaB_elements_AtomicVolume  formulaA_elements_AtomicWeight  \\\n",
       "0                        17.075648                           227.0   \n",
       "1                        16.594425                           227.0   \n",
       "5                        42.527825                           227.0   \n",
       "10                       26.082658                           227.0   \n",
       "28                       28.640877                           227.0   \n",
       "\n",
       "    formulaB_elements_AtomicWeight  formulaA_elements_BoilingT  \\\n",
       "0                       107.868200                      3473.0   \n",
       "1                        26.981539                      3473.0   \n",
       "5                        79.904000                      3473.0   \n",
       "10                      114.818000                      3473.0   \n",
       "28                      204.383300                      3473.0   \n",
       "\n",
       "    formulaB_elements_BoilingT  formulaA_elements_BulkModulus  \\\n",
       "0                       2435.0                            0.0   \n",
       "1                       2792.0                            0.0   \n",
       "5                        332.0                            0.0   \n",
       "10                      2345.0                            0.0   \n",
       "28                      1746.0                            0.0   \n",
       "\n",
       "    formulaB_elements_BulkModulus       ...         A82B  A73B  A64B  A55B  \\\n",
       "0                           100.0       ...            0     1     0     1   \n",
       "1                            76.0       ...            0     1     0     0   \n",
       "5                             1.9       ...            0     1     0     0   \n",
       "10                            0.0       ...            0     1     0     0   \n",
       "28                           43.0       ...            0     0     0     0   \n",
       "\n",
       "    A46B  A37B  A28B  A19B  B  Stable_compunds  \n",
       "0      0     0     0     0  1                1  \n",
       "1      0     0     0     0  1                1  \n",
       "5      0     0     0     0  1                1  \n",
       "10     0     0     1     0  1                1  \n",
       "28     0     0     1     0  1                1  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "\n",
    "df_stable=df_train.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "print(df_stable.shape)\n",
    "df_stable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2572, 98) (2572,)\n"
     ]
    }
   ],
   "source": [
    "#X_train_new=df_stable[feature_cols] #training only on stable elements\n",
    "#y_target=df_stable[stab_vec_list]\n",
    "###\n",
    "\n",
    "X_train_new=df_train[feature_cols]   #training  on all elements\n",
    "y_target=df_train[stab_vec_list[4]]\n",
    "\n",
    "\n",
    "print(X_train_new.shape,y_target.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1988\n",
       "1     584\n",
       "Name: A55B, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2572, 98)\n",
      "(2572, 98)\n",
      "(2572, 98)\n",
      "(2572, 98)\n",
      "(1228, 20)\n",
      "(2572, 98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "X_train_new_mag_1=pd.DataFrame(data=X_train_new_mag_1,columns=feature_cols)\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "X_train_new_0_1=pd.DataFrame(data=X_train_new_0_1,columns=feature_cols)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "X_train_new_m1_p1=pd.DataFrame(data=X_train_new_m1_p1,columns=feature_cols)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "\n",
    "## Taking Logarithm of High Values\n",
    "\n",
    "X_train_new_log=X_train_new.copy()\n",
    "X_train_new_log[X_train_new_log>100]=X_train_new_log[X_train_new_log>100].apply(np.log)\n",
    "print(X_train_new_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: A91B, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(98, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(60, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(30, 15),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(15, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, batch):\n",
    "        return self.layers(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")#device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Network().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2186, 98) (2186,)\n",
      "(386, 98) (386,)\n"
     ]
    }
   ],
   "source": [
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new_m1_p1, y_target,\n",
    "                                                    test_size=.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y1, y2):\n",
    "    \"\"\"standard MSE definition\"\"\"\n",
    "    return ((y1 - y2) ** 2).sum() / y1.data.nelement() * 500\n",
    "\n",
    "\n",
    "def class_loss(y1, y2):\n",
    "    \"\"\"Standard MAE definition\"\"\"\n",
    "    return ((y1 - y2).abs()).sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of steps per epoch: 35\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "num_epochs = 20\n",
    "learning_rate = .00001\n",
    "criterion = nn.BCELoss()\n",
    "#criterion = class_loss\n",
    "#criterion = nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "total_step = len(trainloader)\n",
    "print(\"No. of steps per epoch: %d\" % total_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [0/35], Loss: 0.000000, Val Loss: 0.172694\n",
      "Epoch [1/20], Step [0/35], Loss: 0.000000, Val Loss: 0.345388\n",
      "Epoch [1/20], Step [0/35], Loss: 0.000000, Val Loss: 0.518082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/torch/nn/functional.py:1594: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [0/35], Loss: 0.000000, Val Loss: 0.690776\n",
      "Epoch [2/20], Step [0/35], Loss: 0.431735, Val Loss: 0.086347\n",
      "Epoch [2/20], Step [0/35], Loss: 0.431735, Val Loss: 0.086347\n",
      "Epoch [2/20], Step [0/35], Loss: 0.431735, Val Loss: 0.345388\n",
      "Epoch [2/20], Step [0/35], Loss: 0.431735, Val Loss: 0.518082\n",
      "Epoch [3/20], Step [0/35], Loss: 0.000000, Val Loss: 0.259041\n",
      "Epoch [3/20], Step [0/35], Loss: 0.000000, Val Loss: 0.259041\n",
      "Epoch [3/20], Step [0/35], Loss: 0.000000, Val Loss: 0.345388\n",
      "Epoch [3/20], Step [0/35], Loss: 0.000000, Val Loss: 0.431735\n",
      "Epoch [4/20], Step [0/35], Loss: 0.431735, Val Loss: 0.086347\n",
      "Epoch [4/20], Step [0/35], Loss: 0.431735, Val Loss: 0.259041\n",
      "Epoch [4/20], Step [0/35], Loss: 0.431735, Val Loss: 0.259041\n",
      "Epoch [4/20], Step [0/35], Loss: 0.431735, Val Loss: 0.259041\n",
      "Epoch [5/20], Step [0/35], Loss: 0.000000, Val Loss: 0.172694\n",
      "Epoch [5/20], Step [0/35], Loss: 0.000000, Val Loss: 0.345388\n",
      "Epoch [5/20], Step [0/35], Loss: 0.000000, Val Loss: 0.690776\n",
      "Epoch [5/20], Step [0/35], Loss: 0.000000, Val Loss: 0.690776\n",
      "Epoch [6/20], Step [0/35], Loss: 0.000000, Val Loss: 0.086347\n",
      "Epoch [6/20], Step [0/35], Loss: 0.000000, Val Loss: 0.086347\n",
      "Epoch [6/20], Step [0/35], Loss: 0.000000, Val Loss: 0.172694\n",
      "Epoch [6/20], Step [0/35], Loss: 0.000000, Val Loss: 0.172694\n",
      "Epoch [7/20], Step [0/35], Loss: 0.431735, Val Loss: 0.000000\n",
      "Epoch [7/20], Step [0/35], Loss: 0.431735, Val Loss: 0.345388\n",
      "Epoch [7/20], Step [0/35], Loss: 0.431735, Val Loss: 0.431735\n",
      "Epoch [7/20], Step [0/35], Loss: 0.431735, Val Loss: 0.518082\n",
      "Epoch [8/20], Step [0/35], Loss: 0.000000, Val Loss: 0.000000\n",
      "Epoch [8/20], Step [0/35], Loss: 0.000000, Val Loss: 0.086347\n",
      "Epoch [8/20], Step [0/35], Loss: 0.000000, Val Loss: 0.345388\n",
      "Epoch [8/20], Step [0/35], Loss: 0.000000, Val Loss: 0.345388\n",
      "Epoch [9/20], Step [0/35], Loss: 0.000000, Val Loss: 0.259041\n",
      "Epoch [9/20], Step [0/35], Loss: 0.000000, Val Loss: 0.345388\n",
      "Epoch [9/20], Step [0/35], Loss: 0.000000, Val Loss: 0.518082\n",
      "Epoch [9/20], Step [0/35], Loss: 0.000000, Val Loss: 0.518082\n",
      "Epoch [10/20], Step [0/35], Loss: 0.431735, Val Loss: 0.086347\n",
      "Epoch [10/20], Step [0/35], Loss: 0.431735, Val Loss: 0.259041\n",
      "Epoch [10/20], Step [0/35], Loss: 0.431735, Val Loss: 0.431735\n",
      "Epoch [10/20], Step [0/35], Loss: 0.431735, Val Loss: 0.604429\n"
     ]
    }
   ],
   "source": [
    "print(criterion)\n",
    "print(optimizer)\n",
    "for epoch in range(10):\n",
    "    if epoch > 0:\n",
    "        learning_rate = .00001\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif epoch >= 10:\n",
    "        learning_rate = 0.0002\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif epoch > 80:\n",
    "        learning_rate = 0.0001\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for i, item in enumerate(trainloader):\n",
    "        model.train()\n",
    "        x = item['features'].to(device)\n",
    "        target = item['outputs'].to(device)\n",
    "        # Forward pass\n",
    "        output = model(x)\n",
    "        output1=Variable(data=(output>.9).float(), requires_grad=True)\n",
    "        loss = criterion(output1, target)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 35 == 0:\n",
    "            niter = 5\n",
    "            val_loss=0\n",
    "            for i1, item1 in enumerate(valloader):\n",
    "                x_val = item1['features'].to(device)\n",
    "                y_val = item1['outputs'].to(device)\n",
    "                model.eval()\n",
    "                y_pre = model(x_val)\n",
    "                y_pre1=Variable(data=(y_pre>.9).float(), requires_grad=True)\n",
    "                val_loss += criterion(y_pre1, y_val)\n",
    "\n",
    "                if i1 == niter-1:\n",
    "                    break\n",
    "                if val_loss.item()/niter < 6.5:\n",
    "                    torch.save(model.state_dict(), \"learnt_weights_C0_%d_%d\" % (i, epoch))\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.6f}, Val Loss: {:.6f}' \n",
    "                   .format(epoch+1, num_epochs, i, total_step, loss.item(), val_loss.item()/niter))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument_group('Optimization related arguments')\n",
    "parser.add_argument('-num_epochs', default=100, type=int, help='Epochs')\n",
    "parser.add_argument('-batch_size', default=20, type=int, help='Batch size')\n",
    "parser.add_argument('-lr', default=1e-3, type=float, help='Learning rate')\n",
    "parser.add_argument('-lr_decay_rate', default=0.9997592083, type=float, help='Decay for lr')\n",
    "parser.add_argument('-min_lr', default=5e-5, type=float, help='Minimum learning rate')\n",
    "parser.add_argument('-weight_init', default='xavier', choices=['xavier', 'kaiming'], help='Weight initialization strategy')\n",
    "parser.add_argument('-overfit', action='store_true', help='Overfit on 5 examples, meant for debugging')\n",
    "parser.add_argument('-gpuid', default=-1, type=int, help='GPU id to use')\n",
    "        \n",
    "parser.add_argument('-test_size', default=0.15)\n",
    "\n",
    "parser.add_argument_group('Checkpointing related arguments')\n",
    "parser.add_argument('-load_path', default='', help='Checkpoint to load path from')\n",
    "parser.add_argument('-save_path', default='checkpoints/', help='Path to save checkpoints')\n",
    "parser.add_argument('-save_step', default=2, type=int, help='Save checkpoint after every save_step epochs')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# input arguments and options\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime.strftime(datetime.datetime.utcnow(), '%d-%b-%Y-%H:%M:%S')\n",
    "if args.save_path == 'checkpoints/':\n",
    "    args.save_path += start_time\n",
    "    \n",
    "if args.load_path != '':\n",
    "    components = torch.load(args.load_path)\n",
    "    model_args = components['model_args']\n",
    "    model_args.gpuid = args.gpuid\n",
    "    model_args.batch_size = args.batch_size\n",
    "\n",
    "    # this is required by dataloader\n",
    "    args.normalize = model_args.normalize\n",
    "\n",
    "for arg in vars(args):\n",
    "    print('{:<20}: {}'.format(arg, getattr(args, arg)))\n",
    "    \n",
    "# transfer all options to model\n",
    "model_args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_dataset(Dataset):\n",
    "    \"\"\"Face landmarks Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data, y_true,args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"        \n",
    "        self.data=data\n",
    "        self.y_true=y_true\n",
    "        self.args = args\n",
    "        assert len(data) == len(y_true)\n",
    "        \n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, y_true,\n",
    "                                                            test_size=args.test_size,\n",
    "                                                            shuffle=True,\n",
    "                                                            random_state=42)\n",
    "        self.data = data\n",
    "        self.y_true = y_true\n",
    "\n",
    "        self.X_train = torch.from_numpy(X_train.values).float()\n",
    "        self.y_train = torch.from_numpy(y_train.values).float()\n",
    "        self.X_test = torch.from_numpy(X_test.values).float()\n",
    "        self.y_test = torch.from_numpy(y_test.values).float()\n",
    "\n",
    "        self.num_data_points = {}\n",
    "        self.num_data_points['train'] = len(X_train)\n",
    "        self.num_data_points['test'] = len(X_test)\n",
    "        \n",
    "        self._split = 'train'\n",
    "\n",
    "    @property\n",
    "    def split(self):\n",
    "        return self._split\n",
    "\n",
    "    @split.setter\n",
    "    def split(self, split):\n",
    "        self._split = split\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # methods to override - __len__ and __getitem__ methods\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data_points[self._split]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dtype = self._split\n",
    "        item = {'index': idx}\n",
    "        item['features'] = self.X_train[idx,:]\n",
    "        item['outputs'] = self.y_train[idx]\n",
    "        return item\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "    # collate function utilized by dataloader for batching\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        dtype = self._split\n",
    "        merged_batch = {key: [d[key] for d in batch] for key in batch[0]}\n",
    "        out = {}\n",
    "        for key in merged_batch:\n",
    "            if key in {'index'}:\n",
    "                out[key] = merged_batch[key]\n",
    "            else:\n",
    "                out[key] = torch.stack(merged_batch[key], 0)\n",
    "\n",
    "        batch_keys = ['features', 'outputs']\n",
    "        return {key: out[key] for key in batch_keys}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = arg_dataset(X_train_new_m1_p1, y_target,args)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setattr(args, 'iter_per_epoch', math.ceil(dataset.num_data_points['train'] / args.batch_size))\n",
    "print(\"{} iter per epoch.\".format(args.iter_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(model_args)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.lr_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# training\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "net.train()\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "\n",
    "running_loss = 0.0\n",
    "train_begin = datetime.datetime.utcnow()\n",
    "print(\"Training start time: {}\".format(datetime.datetime.strftime(train_begin, '%d-%b-%Y-%H:%M:%S')))\n",
    "\n",
    "log_loss = []\n",
    "for epoch in range(1, model_args.num_epochs + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        for key in batch:\n",
    "            batch[key] = Variable(batch[key])\n",
    "            if args.gpuid >= 0:\n",
    "                batch[key] = batch[key].cuda()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # forward-backward pass and optimizer step\n",
    "        # --------------------------------------------------------------------\n",
    "        net_out = net(batch['features'])\n",
    "\n",
    "        cur_loss = criterion(net_out, batch['outputs'])\n",
    "        cur_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        gc.collect()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # update running loss and decay learning rates\n",
    "        # --------------------------------------------------------------------\n",
    "        #train_loss = cur_loss.data[0]\n",
    "        train_loss = cur_loss.item()\n",
    "        if running_loss > 0.0:\n",
    "            running_loss = 0.95 * running_loss + 0.05 * train_loss\n",
    "        else:\n",
    "            running_loss = train_loss \n",
    "\n",
    "        if optimizer.param_groups[0]['lr'] > args.min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # print after every few iterations\n",
    "        # --------------------------------------------------------------------\n",
    "        if i % 40 == 0:\n",
    "            test_losses = []\n",
    "            accuracy = []\n",
    "            for i in range(int(dataset.num_data_points['test']/args.batch_size)):\n",
    "                test_feat = dataset.X_test[i*args.batch_size:(i+1)*args.batch_size, :]\n",
    "                test_labels = dataset.y_test[i*args.batch_size:(i+1)*args.batch_size]\n",
    "                test_feat = Variable(test_feat)\n",
    "                test_labels = Variable(test_labels)\n",
    "                if args.gpuid >= 0:\n",
    "                    test_feat = test_feat.cuda()\n",
    "                    test_labels = test_labels.cuda()\n",
    "                net_out = net(test_feat)\n",
    "                cur_loss = criterion(net_out, test_labels)\n",
    "                test_losses.append(cur_loss.item())\n",
    "                \n",
    "                y_pred = torch.sigmoid(net_out).data > 0.5\n",
    "                y_pred = y_pred.cpu().numpy()\n",
    "                accuracy.append((test_labels.cpu().numpy() == y_pred).all(axis=1))\n",
    "\n",
    "            validation_loss = np.mean(test_losses)\n",
    "            \n",
    "            accuracy = np.mean(accuracy)\n",
    "\n",
    "            iteration = (epoch - 1) * args.iter_per_epoch + i\n",
    "\n",
    "            log_loss.append((epoch,\n",
    "                             iteration,\n",
    "                             running_loss,\n",
    "                             train_loss,\n",
    "                             validation_loss,\n",
    "                             accuracy,\n",
    "                             optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            # print current time, running average, learning rate, iteration, epoch\n",
    "            print(\"[{}][Epoch: {:3d}][Iter: {:6d}][Loss: {:6f}][val loss: {:6f}][accuracy: {:6f}][lr: {:7f}]\".format(\n",
    "                datetime.datetime.utcnow() - train_begin, epoch,\n",
    "                    iteration, running_loss, validation_loss, accuracy,\n",
    "                    optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # save checkpoints and final model\n",
    "    # ------------------------------------------------------------------------\n",
    "    if epoch % args.save_step == 0:\n",
    "        torch.save({\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': net.args\n",
    "        }, os.path.join(args.save_path, 'model_epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "torch.save({\n",
    "    'net': net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_args': net.args\n",
    "}, os.path.join(args.save_path, 'model_final.pth'))\n",
    "\n",
    "np.save(os.path.join(args.save_path, 'log_loss'), log_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class feature_Dataset(Dataset):\n",
    "    \"\"\"Face landmarks Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, X_features, y_target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"        \n",
    "        self.X_features=X_features\n",
    "        self.y_target=y_target\n",
    "        assert len(X_features) == len(y_target)\n",
    "        \n",
    "\n",
    "\n",
    "        self.X_features = torch.from_numpy(X_features.values).float()\n",
    "        self.y_target = torch.from_numpy(y_target.values).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        \n",
    "        item = {'index': idx}\n",
    "        item['features'] = self.X_features[idx,:]\n",
    "        item['outputs'] = self.y_target[idx]\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataset = feature_Dataset(X_train, y_train) ## Now you can feed any X-train and Y-train. normalize it and feed the highest pearson correlated one\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "\n",
    "val_dataset = feature_Dataset(X_test, y_test)\n",
    "\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  tensor([[ 0.8387,  0.2151,  0.0004,  0.0012,  0.8196,  0.1684,  0.6982,  0.2994,\n",
      "          0.6053,  0.0447,  0.5556,  0.1111,  0.5574,  0.7213,  0.9336,  0.0686,\n",
      "          0.9622,  0.4919,  0.7600,  0.3333,  0.3644,  0.2486,  0.0000,  0.0000,\n",
      "         -0.4652, -0.1503,  0.5062,  0.6884,  0.5062,  0.6884,  0.0000,  0.0000,\n",
      "          0.1297,  0.3263,  0.5612,  0.3980,  1.0000,  0.1648,  0.0259,  0.1261,\n",
      "          0.3417,  0.3426,  0.1889,  0.0727,  0.1622,  0.4747,  0.0000,  1.0000,\n",
      "          1.0000,  0.0000,  0.0000,  0.0000,  1.0000,  1.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.5340,  0.2917,  0.6176,  0.0686,  0.5265,  0.7614,\n",
      "          0.0909,  0.0000,  0.8276,  0.0690,  0.1111,  0.0000,  0.9000,  0.0000,\n",
      "          0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000,  0.5000,  1.0000,  0.8387,  0.2151,  0.1094,  0.4331,\n",
      "          0.8571,  0.5714,  0.2748,  0.0333,  0.9825,  0.9825,  1.0000,  1.0000,\n",
      "          0.5282,  0.7319]])\n",
      "outputs:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "item = train_dataset[0]\n",
    "\n",
    "x = item['features'][None]\n",
    "print(\"features: \", x)\n",
    "y = item['outputs']\n",
    "print(\"outputs: \", y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
