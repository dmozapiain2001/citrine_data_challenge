{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data is being read ....\n",
      "(2572, 98)\n",
      "(2572, 109)\n",
      "Training Data has been read and feature engineering is being performed....\n",
      "(2572, 110)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formulaA</th>\n",
       "      <th>formulaB</th>\n",
       "      <th>formulaA_elements_AtomicVolume</th>\n",
       "      <th>formulaB_elements_AtomicVolume</th>\n",
       "      <th>formulaA_elements_AtomicWeight</th>\n",
       "      <th>formulaB_elements_AtomicWeight</th>\n",
       "      <th>formulaA_elements_BoilingT</th>\n",
       "      <th>formulaB_elements_BoilingT</th>\n",
       "      <th>formulaA_elements_BulkModulus</th>\n",
       "      <th>formulaB_elements_BulkModulus</th>\n",
       "      <th>...</th>\n",
       "      <th>A82B</th>\n",
       "      <th>A73B</th>\n",
       "      <th>A64B</th>\n",
       "      <th>A55B</th>\n",
       "      <th>A46B</th>\n",
       "      <th>A37B</th>\n",
       "      <th>A28B</th>\n",
       "      <th>A19B</th>\n",
       "      <th>B</th>\n",
       "      <th>Stable_compunds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>47</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>17.075648</td>\n",
       "      <td>227.0</td>\n",
       "      <td>107.868200</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>16.594425</td>\n",
       "      <td>227.0</td>\n",
       "      <td>26.981539</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>33</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>21.723966</td>\n",
       "      <td>227.0</td>\n",
       "      <td>74.921600</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89</td>\n",
       "      <td>56</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>64.969282</td>\n",
       "      <td>227.0</td>\n",
       "      <td>137.327000</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89</td>\n",
       "      <td>83</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>35.483459</td>\n",
       "      <td>227.0</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   formulaA  formulaB  formulaA_elements_AtomicVolume  \\\n",
       "0        89        47                       37.433086   \n",
       "1        89        13                       37.433086   \n",
       "2        89        33                       37.433086   \n",
       "3        89        56                       37.433086   \n",
       "4        89        83                       37.433086   \n",
       "\n",
       "   formulaB_elements_AtomicVolume  formulaA_elements_AtomicWeight  \\\n",
       "0                       17.075648                           227.0   \n",
       "1                       16.594425                           227.0   \n",
       "2                       21.723966                           227.0   \n",
       "3                       64.969282                           227.0   \n",
       "4                       35.483459                           227.0   \n",
       "\n",
       "   formulaB_elements_AtomicWeight  formulaA_elements_BoilingT  \\\n",
       "0                      107.868200                      3473.0   \n",
       "1                       26.981539                      3473.0   \n",
       "2                       74.921600                      3473.0   \n",
       "3                      137.327000                      3473.0   \n",
       "4                      208.980400                      3473.0   \n",
       "\n",
       "   formulaB_elements_BoilingT  formulaA_elements_BulkModulus  \\\n",
       "0                      2435.0                            0.0   \n",
       "1                      2792.0                            0.0   \n",
       "2                       887.0                            0.0   \n",
       "3                      2143.0                            0.0   \n",
       "4                      1837.0                            0.0   \n",
       "\n",
       "   formulaB_elements_BulkModulus       ...         A82B  A73B  A64B  A55B  \\\n",
       "0                          100.0       ...            0     1     0     1   \n",
       "1                           76.0       ...            0     1     0     0   \n",
       "2                           22.0       ...            0     0     0     0   \n",
       "3                            9.6       ...            0     0     0     0   \n",
       "4                           31.0       ...            0     0     0     0   \n",
       "\n",
       "   A46B  A37B  A28B  A19B  B  Stable_compunds  \n",
       "0     0     0     0     0  1                1  \n",
       "1     0     0     0     0  1                1  \n",
       "2     0     0     0     0  1                0  \n",
       "3     0     0     0     0  1                0  \n",
       "4     0     0     0     0  1                0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # Reading in the Data\n",
    "\n",
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "        \n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "df_train.shape\n",
    "\n",
    "\n",
    "# ## Data Manipulation\n",
    "print('Training Data is being read ....')\n",
    "#  - Transforming the outcome to a numpy vector\n",
    "\n",
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "# ### Input Data Normalization and Feature Engineering\n",
    "print('Training Data has been read and feature engineering is being performed....')\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_data.csv', 'test_data.csv']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Output for Component 1 of Stability Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2512\n",
      "1      60\n",
      "Name: A19B, dtype: int64\n",
      "Compound being analyzed is A19B\n",
      "0    1240\n",
      "1      60\n",
      "Name: A19B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector: (1300,)\n",
      "0    545\n",
      "1     60\n",
      "Name: A19B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound: (605,)\n"
     ]
    }
   ],
   "source": [
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "count=8\n",
    "    \n",
    "y = df_train[stab_vec_list[count]]\n",
    "print(y.value_counts())\n",
    "\n",
    "stable_comp=df_train.loc[y==1,['formulaA','formulaB']] # Find the elements that create a stable element in this vector component\n",
    "print('Compound being analyzed is',stab_vec_list[count])\n",
    "stable_comp_num=stable_comp.values\n",
    "stable_A=np.unique(stable_comp_num[:,0])\n",
    "stable_B=np.unique(stable_comp_num[:,1])\n",
    "    \n",
    "df_unique= pd.DataFrame()\n",
    "\n",
    "y_unique= pd.DataFrame()\n",
    "    \n",
    "for cnt in range(stable_A.shape[0]):\n",
    "\n",
    "    df_tmp1=y.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "    y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "    df_tmp=df_train.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "    df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for cnt in range(stable_B.shape[0]):\n",
    "    df_tmp1=y.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "    y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "    df_tmp=df_train.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "    df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "\n",
    "    \n",
    "y_unique=y.iloc[y_unique.index.unique()]\n",
    "df_unique=df_train.iloc[df_unique.index.unique()]\n",
    "print(y_unique.value_counts())\n",
    "print('The elements in these compounds create a stable compound for this component of the stability vector:',y_unique.shape)\n",
    "    \n",
    "    \n",
    "y_stable=y_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "df_stable=df_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "print(y_stable.value_counts())\n",
    "print('The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound:',y_stable.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation and Input Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation has been calculated to build the model in the most relevant features ....\n",
      "Number of Results to train on: (605,)\n",
      "Number of Training Features before Pearson correlation: 98\n",
      "Pearson Correlation has identified 30 with  0.13\n",
      "Number of Training Features after Pearson correlation: 30\n",
      "(605, 30)\n",
      "(605, 30)\n",
      "(605, 30)\n",
      "(605, 30)\n",
      "(605, 20)\n",
      "(605, 20)\n",
      "Pearson Correlation in PCA Space has identified 12 with  0.05\n",
      "Number of Training Features after Pearson correlation in PCA Space: 12\n"
     ]
    }
   ],
   "source": [
    "# Pearson Correlation to Identify the features that influence the most on the output \n",
    "print('Pearson Correlation has been calculated to build the model in the most relevant features ....')\n",
    "X_train_new_all=df_stable[feature_cols] #This means we will only train on the elements that create a stable compound for this component of the stability vector and have at least one stable compound\n",
    "\n",
    "y_new=y_stable\n",
    "print('Number of Results to train on:',y_new.shape)\n",
    "print('Number of Training Features before Pearson correlation:', X_train_new_all.shape[1])\n",
    "\n",
    "corr_df=pd.concat([X_train_new_all, y_new],axis=1)\n",
    "a=corr_df.corr()\n",
    "#a['Stable_compunds'].hist(bins=7, figsize=(18, 12), xlabelsize=10)\n",
    "\n",
    "## Incorporating the Features that contribute the most based on a pearson correlation coefficient threshold\n",
    "\n",
    "thr=.13\n",
    "\n",
    "corr_variables=list(a[a[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "del(corr_variables[-1])\n",
    "\n",
    "\n",
    "print('Pearson Correlation has identified', len(corr_variables), 'with ', str(thr) )\n",
    "\n",
    "## Normalization of Input Data\n",
    "\n",
    "## Using Un-normalized data as input\n",
    "X_train_new=df_stable[corr_variables]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "\n",
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "## Using Pearson Correlation in PCA\n",
    "df1= pd.DataFrame(data=X_train_new_PCA, index=indx_4_PC)\n",
    "print(df1.shape)\n",
    "\n",
    "corr_df_PCA=pd.concat([df1, y_new],axis=1)\n",
    "\n",
    "\n",
    "a_PCA=corr_df_PCA.corr()\n",
    "\n",
    "thr=.05\n",
    "corr_variables_PCA=list(a_PCA[a_PCA[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "\n",
    "del(corr_variables_PCA[-1])\n",
    "\n",
    "print('Pearson Correlation in PCA Space has identified', len(corr_variables_PCA), 'with ', str(thr) )\n",
    "\n",
    "X_train_PCA_PC=df1[corr_variables_PCA]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation in PCA Space:', X_train_PCA_PC.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model Using Z-normalized Data\n",
      "(1040, 29) (1040,)\n",
      "(184, 29) (184,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Model Using Z-normalized Data')\n",
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new_Z_score, y_new,\n",
    "                                                    test_size=.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4894230769230769\n"
     ]
    }
   ],
   "source": [
    "print(y_train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Hyper Parameter Tuning to identify best Classifier and its values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- Random Forest --')\n",
    "\n",
    "#first pass\n",
    "n_estimators = [1,3,5,10,50,100]\n",
    "criterion=['entropy']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[2,5,10]\n",
    "\n",
    "min_samples_splits=[2,3,4,6,7,8,9,10,20]\n",
    "min_samples_leafs=[1,2,5,10]\n",
    "min_impurity_splits=[5e-7 ,1e-6]\n",
    "\n",
    "#second pass\n",
    "#n_estimators = [10,20,50]\n",
    "#criterion=['entropy']\n",
    "#bootstrap= [True, False]\n",
    "#max_depth=[5,6]\n",
    "#min_samples_splits=[2,3,4,5,6]\n",
    "#min_samples_leafs=[1,3,5]\n",
    "#min_impurity_splits=[3e-7, 5e-7,1e-6]\n",
    "\n",
    "#n_estimators = [1,3,5,8]\n",
    "#criterion=['entropy']\n",
    "#bootstrap= [True, False]\n",
    "#max_depth=[1,3,4]\n",
    "\n",
    "\n",
    "#min_samples_splits=[2,3,4,5]\n",
    "#min_samples_leafs=[1]\n",
    "#min_impurity_splits=[3e-7, 5e-7,8e-7]\n",
    "\n",
    "df_results_RF=scores.hp_tune_Random_Forest(X_train,y_train,X_test,y_test,2,n_estimators,criterion,bootstrap,max_depth,min_samples_splits,min_samples_leafs,min_impurity_splits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('This are the best Parameters for Random Forest:')\n",
    "print(df_results_RF[df_results_RF['test_accuracy']==df_results_RF['test_accuracy'].max()].head())\n",
    "\n",
    "\n",
    "# # Decision Trees\n",
    "\n",
    "\n",
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- Decision Trees --')\n",
    "\n",
    "\n",
    "criterion=['entropy','gini']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[1,2,5,10,100,250,1000]\n",
    "split=['random','best']\n",
    "min_samples_splits=[2,3,4,6,7,8,9,10]\n",
    "min_samples_leafs=[1]\n",
    "min_impurity_splits=[5e-7 ,1e-6]\n",
    "\n",
    "#second pass\n",
    "#criterion=['entropy']\n",
    "#max_depth=[10,11,15]\n",
    "#split=['random','best']\n",
    "#min_samples_splits=[2,3,4,6]\n",
    "#min_samples_leafs=[1,3,5]\n",
    "#min_impurity_splits=[3e-7, 5e-7,1e-6]\n",
    "\n",
    "#criterion=['entropy']\n",
    "#max_depth=[1,3,510]\n",
    "#split=['best']\n",
    "#min_samples_splits=[2,3]\n",
    "#min_samples_leafs=[1]\n",
    "#min_impurity_splits=[3e-7, 5e-7,8e-5]\n",
    "\n",
    "df_results_DT=scores.hp_tune_Decision_tree(X_train,y_train,X_test,y_test,2,criterion,max_depth,split,min_samples_splits,min_samples_leafs,min_impurity_splits)\n",
    "\n",
    "print('This are the best Parameters for Decision Tree:')\n",
    "print(df_results_DT[df_results_DT['test_results_auc']==df_results_DT['test_results_auc'].max()].head())\n",
    "\n",
    "\n",
    "\n",
    "# # KNN \n",
    "\n",
    "\n",
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- KNN --')\n",
    "\n",
    "criterion=['distance', 'uniform']\n",
    "neighbors=[1,2,3,10,50,100]\n",
    "distances = [1, 2, 3, 4, 5]\n",
    "\n",
    "df_results_KNN=scores.hp_tune_KNN(X_train,y_train,X_test,y_test,2,criterion,neighbors,distances)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('This are the best Parameters for KNN :')\n",
    "print(df_results_KNN[df_results_KNN['test_results_auc']==df_results_KNN['test_results_auc'].max()].head())\n",
    "\n",
    "\n",
    "# # SVM\n",
    "\n",
    "\n",
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- SVM --')\n",
    "\n",
    "kernel=['rbf', 'linear', 'poly', 'sigmoid']\n",
    "gammas = [.001,.1,1,3,5]\n",
    "cs = [.0001,.1,1,5,10,15,20]\n",
    "\n",
    "df_results_SVM=scores.hp_tune_SVM(X_train,y_train,X_test,y_test,10,kernel,gammas,cs)\n",
    "\n",
    "\n",
    "\n",
    "print('This are the best Parameters for SVM :')\n",
    "print(df_results_SVM[df_results_SVM['test_results_auc']==df_results_SVM['test_results_auc'].max()].head())\n",
    "\n",
    "\n",
    "# # Logistic Regression\n",
    "\n",
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- Logistic Regression --')\n",
    "\n",
    "criterion=['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "\n",
    "df_results_log_reg=scores.hp_tune_log_reg(X_train,y_train,X_test,y_test,10,criterion)\n",
    "\n",
    "print('This are the best Parameters for Logistic Regression :')\n",
    "print(df_results_log_reg[df_results_log_reg['test_results_auc']==df_results_log_reg['test_results_auc'].max()].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Optimal Random Forest --\n",
      "Training precision:  0.7241379310344828   recall:  1.0   F1:  0.8400000000000001   accuracy:  0.9700934579439252\n",
      "Training Confusion matrix\n",
      "[[477  16]\n",
      " [  0  42]]\n",
      "Training AUC: 0.9837728194726166\n",
      "Optimal precision:  0.4   recall:  0.5   F1:  0.4444444444444445   accuracy:  0.8947368421052632\n",
      "optimal Confusion matrix\n",
      "[[81  6]\n",
      " [ 4  4]]\n",
      "Optimal AUC: 0.7155172413793104\n",
      " -- Default Random Forest --\n",
      "DEF Training precision:  1.0   recall:  0.8809523809523809   F1:  0.9367088607594937   accuracy:  0.9906542056074766\n",
      "DEF Training Confusion matrix\n",
      "[[493   0]\n",
      " [  5  37]]\n",
      "DEF Training AUC: 0.9404761904761905\n",
      "Defualt Model precision:  0.6   recall:  0.375   F1:  0.4615384615384615   accuracy:  0.9263157894736842\n",
      "Defualt ModelConfusion matrix\n",
      "[[85  2]\n",
      " [ 5  3]]\n",
      "Defualt ModelAUC: 0.6760057471264368\n"
     ]
    }
   ],
   "source": [
    "## Fitting best Model\n",
    "print(' -- Optimal Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_opt = RandomForestClassifier(n_estimators=100,criterion='gini',bootstrap=False,max_depth=10, \n",
    "                                 min_samples_split=10,\n",
    "                                 min_samples_leaf=5,\n",
    "                                 min_impurity_decrease=5e-07,\n",
    "                                 random_state=0\n",
    "                                 ,n_jobs=-1,class_weight={0:.1, 1:0.9})\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "\n",
    "train_pred = rfc_opt.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)\n",
    "\n",
    "\n",
    "y_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "## Compare to Default Model\n",
    "print(' -- Default Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_def = RandomForestClassifier()\n",
    "rfc_def.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "train_pred = rfc_def.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('DEF Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('DEF Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('DEF Training AUC:',roc_auc)\n",
    "\n",
    "y_pred = rfc_def.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting best Model\n",
    "print(' -- Optimal Decision Tree --')\n",
    "## Fitting best Model\n",
    "\n",
    "rfc_opt = sklearn.tree.DecisionTreeClassifier(class_weight={0:1-y_train.mean(), 1:y_train.mean()},\n",
    "                                          criterion='entropy',max_depth=510,\n",
    "                                          random_state=0, \n",
    "                                          splitter='best',\n",
    "                                          min_samples_split=3,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          min_impurity_decrease=3e-7)\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "\n",
    "train_pred = rfc_opt.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)\n",
    "\n",
    "\n",
    "y_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "## Compare to Default Model\n",
    "print(' -- Default Decision Tree --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_def = sklearn.tree.DecisionTreeClassifier()\n",
    "\n",
    "rfc_def.fit(X_train, y_train)\n",
    "train_pred = rfc_def.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('DEF Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('DEF Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('DEF Training AUC:',roc_auc)\n",
    "\n",
    "y_pred = rfc_def.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting best Model\n",
    "print(' -- Optimal SVM --')\n",
    "import sklearn.svm\n",
    "\n",
    "rfc = sklearn.svm.SVC(kernel='poly', gamma=3,C=10,random_state=0,class_weight={0:1-y_train.mean(), 1:y_train.mean()})\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print(' -- Defualt SVM --')\n",
    "\n",
    "rfc = sklearn.svm.SVC()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting best Model\n",
    "print(' -- Optimal KNN --')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "rfc = KNeighborsClassifier(algorithm='auto',metric='minkowski',n_jobs=-1, n_neighbors=1, p=3,weights='distance')\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print(' -- Defualt KNN --')\n",
    "\n",
    "rfc = KNeighborsClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Bagging of Best Performing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print('------- Extra Trees Classifier-------')\n",
    "clf_ex = ExtraTreesClassifier(class_weight={0:1-y_train.mean(), 1:y_train.mean()},\n",
    "                              min_samples_split=2,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 min_impurity_decrease=3e-07)\n",
    "\n",
    "\n",
    "clf_ex.fit(X_train, y_train)\n",
    "\n",
    "train_pred = rfc_opt.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)\n",
    "\n",
    "y_pred = clf_ex.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf_ex.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .1)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADAboosting\n",
    "print('------ ADAboosting with Decision Tree ----')\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "rfc_opt = sklearn.tree.DecisionTreeClassifier(class_weight={0:1-y_train.mean(), 1:y_train.mean()},\n",
    "                                          criterion='entropy',max_depth=10,\n",
    "                                          random_state=0, \n",
    "                                          splitter='best',\n",
    "                                          min_samples_split=2,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          min_impurity_decrease=5e-7)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=rfc_opt, n_estimators=100,learning_rate=.01)\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf,X=X_train, y=y_train, cv=10,scoring='roc_auc')\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .1)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADAboosting\n",
    "print('------ ADAboosting with Random Forest ----')\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "rfc_opt = RandomForestClassifier(n_estimators=50,criterion='entropy',bootstrap=True,max_depth=5, \n",
    "                                 min_samples_split=2,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 min_impurity_decrease=5e-07,\n",
    "                                 random_state=0\n",
    "                                 ,n_jobs=-1)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=rfc_opt, n_estimators=10,learning_rate=.01)\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf,X=X_train, y=y_train, cv=10,scoring='roc_auc')\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .1)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient boosting\n",
    "print('------ Gradient Boosting with Decision Trees ----')\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf  = GradientBoostingClassifier(n_estimators=60, learning_rate=.1,min_samples_split=2,\n",
    "                                          min_samples_leaf=1,\n",
    "                                          min_impurity_decrease=5e-7)\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf,X=X_train, y=y_train, cv=10,scoring='roc_auc')\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .25)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
