{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data is being read ....\n",
      "Training Data has been read and feature engineering is being performed....\n",
      "(2572, 98)\n",
      "(2572, 109)\n",
      "(2572, 110)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formulaA</th>\n",
       "      <th>formulaB</th>\n",
       "      <th>formulaA_elements_AtomicVolume</th>\n",
       "      <th>formulaB_elements_AtomicVolume</th>\n",
       "      <th>formulaA_elements_AtomicWeight</th>\n",
       "      <th>formulaB_elements_AtomicWeight</th>\n",
       "      <th>formulaA_elements_BoilingT</th>\n",
       "      <th>formulaB_elements_BoilingT</th>\n",
       "      <th>formulaA_elements_BulkModulus</th>\n",
       "      <th>formulaB_elements_BulkModulus</th>\n",
       "      <th>...</th>\n",
       "      <th>A82B</th>\n",
       "      <th>A73B</th>\n",
       "      <th>A64B</th>\n",
       "      <th>A55B</th>\n",
       "      <th>A46B</th>\n",
       "      <th>A37B</th>\n",
       "      <th>A28B</th>\n",
       "      <th>A19B</th>\n",
       "      <th>B</th>\n",
       "      <th>Stable_compunds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>47</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>17.075648</td>\n",
       "      <td>227.0</td>\n",
       "      <td>107.868200</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>16.594425</td>\n",
       "      <td>227.0</td>\n",
       "      <td>26.981539</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>33</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>21.723966</td>\n",
       "      <td>227.0</td>\n",
       "      <td>74.921600</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89</td>\n",
       "      <td>56</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>64.969282</td>\n",
       "      <td>227.0</td>\n",
       "      <td>137.327000</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89</td>\n",
       "      <td>83</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>35.483459</td>\n",
       "      <td>227.0</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   formulaA  formulaB  formulaA_elements_AtomicVolume  \\\n",
       "0        89        47                       37.433086   \n",
       "1        89        13                       37.433086   \n",
       "2        89        33                       37.433086   \n",
       "3        89        56                       37.433086   \n",
       "4        89        83                       37.433086   \n",
       "\n",
       "   formulaB_elements_AtomicVolume  formulaA_elements_AtomicWeight  \\\n",
       "0                       17.075648                           227.0   \n",
       "1                       16.594425                           227.0   \n",
       "2                       21.723966                           227.0   \n",
       "3                       64.969282                           227.0   \n",
       "4                       35.483459                           227.0   \n",
       "\n",
       "   formulaB_elements_AtomicWeight  formulaA_elements_BoilingT  \\\n",
       "0                      107.868200                      3473.0   \n",
       "1                       26.981539                      3473.0   \n",
       "2                       74.921600                      3473.0   \n",
       "3                      137.327000                      3473.0   \n",
       "4                      208.980400                      3473.0   \n",
       "\n",
       "   formulaB_elements_BoilingT  formulaA_elements_BulkModulus  \\\n",
       "0                      2435.0                            0.0   \n",
       "1                      2792.0                            0.0   \n",
       "2                       887.0                            0.0   \n",
       "3                      2143.0                            0.0   \n",
       "4                      1837.0                            0.0   \n",
       "\n",
       "   formulaB_elements_BulkModulus       ...         A82B  A73B  A64B  A55B  \\\n",
       "0                          100.0       ...            0     1     0     1   \n",
       "1                           76.0       ...            0     1     0     0   \n",
       "2                           22.0       ...            0     0     0     0   \n",
       "3                            9.6       ...            0     0     0     0   \n",
       "4                           31.0       ...            0     0     0     0   \n",
       "\n",
       "   A46B  A37B  A28B  A19B  B  Stable_compunds  \n",
       "0     0     0     0     0  1                1  \n",
       "1     0     0     0     0  1                1  \n",
       "2     0     0     0     0  1                0  \n",
       "3     0     0     0     0  1                0  \n",
       "4     0     0     0     0  1                0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training Data is being read ....')\n",
    "# # Reading in the Data\n",
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "        \n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "df_train.shape\n",
    "\n",
    "print('Training Data has been read and feature engineering is being performed....')\n",
    "\n",
    "# ## Data Manipulation\n",
    "\n",
    "#  - Transforming the outcome to a numpy vector\n",
    "\n",
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "\n",
    "\n",
    "# ### Input Data Normalization and Feature Engineering\n",
    "\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_data.csv', 'test_data.csv']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now getting the Output for each Vector Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2522\n",
      "1      50\n",
      "Name: A91B, dtype: int64\n",
      "Compound being analyzed is A91B\n",
      "0    1332\n",
      "1      50\n",
      "Name: A91B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector: (1382,)\n",
      "0    580\n",
      "1     50\n",
      "Name: A91B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound: (630,)\n"
     ]
    }
   ],
   "source": [
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "for count in range(0,1):\n",
    "    \n",
    "    y = df_train[stab_vec_list[count]]\n",
    "    print(y.value_counts())\n",
    "\n",
    "    stable_comp=df_train.loc[y==1,['formulaA','formulaB']] # Find the elements that create a stable element in this vector component\n",
    "    print('Compound being analyzed is',stab_vec_list[count])\n",
    "    stable_comp_num=stable_comp.values\n",
    "    stable_A=np.unique(stable_comp_num[:,0])\n",
    "    stable_B=np.unique(stable_comp_num[:,1])\n",
    "    \n",
    "    df_unique= pd.DataFrame()\n",
    "\n",
    "    y_unique= pd.DataFrame()\n",
    "    \n",
    "    for cnt in range(stable_A.shape[0]):\n",
    "\n",
    "        df_tmp1=y.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "        df_tmp=df_train.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "        df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for cnt in range(stable_B.shape[0]):\n",
    "        df_tmp1=y.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "        df_tmp=df_train.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "        df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "\n",
    "    \n",
    "    y_unique=y.iloc[y_unique.index.unique()]\n",
    "    df_unique=df_train.iloc[df_unique.index.unique()]\n",
    "    print(y_unique.value_counts())\n",
    "    print('The elements in these compounds create a stable compound for this component of the stability vector:',y_unique.shape)\n",
    "    \n",
    "    \n",
    "    y_stable=y_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "    df_stable=df_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "    print(y_stable.value_counts())\n",
    "    print('The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound:',y_stable.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formulaA\n",
      "PointbiserialrResult(correlation=-0.011368718042507837, pvalue=0.7758011504442677)\n",
      "formulaB\n",
      "PointbiserialrResult(correlation=-0.15211053710898856, pvalue=0.00012675157839360166)\n",
      "formulaA_elements_AtomicVolume\n",
      "PointbiserialrResult(correlation=0.20424599749109668, pvalue=2.3289463319694915e-07)\n",
      "formulaB_elements_AtomicVolume\n",
      "PointbiserialrResult(correlation=-0.18854346192047858, pvalue=1.8809366323384195e-06)\n",
      "formulaA_elements_AtomicWeight\n",
      "PointbiserialrResult(correlation=-0.011297168374981221, pvalue=0.7771748591598144)\n",
      "formulaB_elements_AtomicWeight\n",
      "PointbiserialrResult(correlation=-0.14567033222709477, pvalue=0.00024386182783526643)\n",
      "formulaA_elements_BoilingT\n",
      "PointbiserialrResult(correlation=-0.04815146302560412, pvalue=0.22747371278890569)\n",
      "formulaB_elements_BoilingT\n",
      "PointbiserialrResult(correlation=0.007295227375598992, pvalue=0.854996110286619)\n",
      "formulaA_elements_BulkModulus\n",
      "PointbiserialrResult(correlation=-0.05250286134157551, pvalue=0.18813977901568635)\n",
      "formulaB_elements_BulkModulus\n",
      "PointbiserialrResult(correlation=0.09422718337634459, pvalue=0.017998353696473644)\n",
      "formulaA_elements_Column\n",
      "PointbiserialrResult(correlation=-0.16218124216281699, pvalue=4.318600235464511e-05)\n",
      "formulaB_elements_Column\n",
      "PointbiserialrResult(correlation=-0.025122611574982412, pvalue=0.5290743820408137)\n",
      "formulaA_elements_CovalentRadius\n",
      "PointbiserialrResult(correlation=0.15594109518570065, pvalue=8.480938579856576e-05)\n",
      "formulaB_elements_CovalentRadius\n",
      "PointbiserialrResult(correlation=-0.20095287057452987, pvalue=3.660231433954431e-07)\n",
      "formulaA_elements_Density\n",
      "PointbiserialrResult(correlation=-0.07366972406304412, pvalue=0.06461177410886221)\n",
      "formulaB_elements_Density\n",
      "PointbiserialrResult(correlation=-0.0800476116928096, pvalue=0.04459944970302871)\n",
      "formulaA_elements_ElectronSurfaceDensityWS\n",
      "PointbiserialrResult(correlation=-0.07497575754058285, pvalue=0.06000088174339072)\n",
      "formulaB_elements_ElectronSurfaceDensityWS\n",
      "PointbiserialrResult(correlation=0.11781873989802384, pvalue=0.0030597030239063188)\n",
      "formulaA_elements_Electronegativity\n",
      "PointbiserialrResult(correlation=-0.1626711507097385, pvalue=4.091352852105532e-05)\n",
      "formulaB_elements_Electronegativity\n",
      "PointbiserialrResult(correlation=-0.05964321147002244, pvalue=0.13481238601493023)\n",
      "formulaA_elements_FirstIonizationEnergy\n",
      "PointbiserialrResult(correlation=-0.16625241315936487, pvalue=2.742768014623857e-05)\n",
      "formulaB_elements_FirstIonizationEnergy\n",
      "PointbiserialrResult(correlation=0.09250107724863703, pvalue=0.02022603661761565)\n",
      "formulaA_elements_GSbandgap\n",
      "PointbiserialrResult(correlation=-0.043487604184723506, pvalue=0.2757668327196881)\n",
      "formulaB_elements_GSbandgap\n",
      "PointbiserialrResult(correlation=0.03933121926660271, pvalue=0.3243163620223687)\n",
      "formulaA_elements_GSenergy_pa\n",
      "PointbiserialrResult(correlation=0.06273403455066494, pvalue=0.1157108616568458)\n",
      "formulaB_elements_GSenergy_pa\n",
      "PointbiserialrResult(correlation=0.06007303163106905, pvalue=0.13201913267773013)\n",
      "formulaA_elements_GSestBCClatcnt\n",
      "PointbiserialrResult(correlation=0.173938861603058, pvalue=1.130006352375008e-05)\n",
      "formulaB_elements_GSestBCClatcnt\n",
      "PointbiserialrResult(correlation=-0.2259021971765088, pvalue=9.860462718732741e-09)\n",
      "formulaA_elements_GSestFCClatcnt\n",
      "PointbiserialrResult(correlation=0.17393886160970015, pvalue=1.130006351490066e-05)\n",
      "formulaB_elements_GSestFCClatcnt\n",
      "PointbiserialrResult(correlation=-0.22590219717952312, pvalue=9.860462714164856e-09)\n",
      "formulaA_elements_GSmagmom\n",
      "PointbiserialrResult(correlation=-0.01604839490611295, pvalue=0.687656467327668)\n",
      "formulaB_elements_GSmagmom\n",
      "PointbiserialrResult(correlation=0.0443818126610674, pvalue=0.2660046713348394)\n",
      "formulaA_elements_GSvolume_pa\n",
      "PointbiserialrResult(correlation=0.20537919958580134, pvalue=1.989939778959071e-07)\n",
      "formulaB_elements_GSvolume_pa\n",
      "PointbiserialrResult(correlation=-0.1874473444934109, pvalue=2.1626056554883477e-06)\n",
      "formulaA_elements_HHIp\n",
      "PointbiserialrResult(correlation=0.010962081639981924, pvalue=0.7836175018414466)\n",
      "formulaB_elements_HHIp\n",
      "PointbiserialrResult(correlation=-0.019128251523639565, pvalue=0.6317923232034603)\n",
      "formulaA_elements_HHIr\n",
      "PointbiserialrResult(correlation=0.018533569142674507, pvalue=0.6424292876519984)\n",
      "formulaB_elements_HHIr\n",
      "PointbiserialrResult(correlation=-0.08998053356867233, pvalue=0.023908816122807)\n",
      "formulaA_elements_HeatCapacityMass\n",
      "PointbiserialrResult(correlation=0.038755496021082, pvalue=0.33145452789860974)\n",
      "formulaB_elements_HeatCapacityMass\n",
      "PointbiserialrResult(correlation=0.18700286900088992, pvalue=2.2879758297499963e-06)\n",
      "formulaA_elements_HeatCapacityMolar\n",
      "PointbiserialrResult(correlation=0.03013458287989575, pvalue=0.45022414362941043)\n",
      "formulaB_elements_HeatCapacityMolar\n",
      "PointbiserialrResult(correlation=-0.13805732182767416, pvalue=0.0005108858741512763)\n",
      "formulaA_elements_HeatFusion\n",
      "PointbiserialrResult(correlation=-0.05599886940023726, pvalue=0.16035731087597158)\n",
      "formulaB_elements_HeatFusion\n",
      "PointbiserialrResult(correlation=0.03928884258612248, pvalue=0.324838343754999)\n",
      "formulaA_elements_ICSDVolume\n",
      "PointbiserialrResult(correlation=0.17777646281156953, pvalue=7.152522857675189e-06)\n",
      "formulaB_elements_ICSDVolume\n",
      "PointbiserialrResult(correlation=-0.1792570928469171, pvalue=5.979833491991519e-06)\n",
      "formulaA_elements_IsAlkali\n",
      "PointbiserialrResult(correlation=0.17789995821609503, pvalue=7.046880586722036e-06)\n",
      "formulaB_elements_IsAlkali\n",
      "PointbiserialrResult(correlation=0.08631766282897237, pvalue=0.030288425448912002)\n",
      "formulaA_elements_IsDBlock\n",
      "PointbiserialrResult(correlation=-0.06043889115676886, pvalue=0.1296769651831565)\n",
      "formulaB_elements_IsDBlock\n",
      "PointbiserialrResult(correlation=0.04914320114765605, pvalue=0.21803155326016418)\n",
      "formulaA_elements_IsFBlock\n",
      "PointbiserialrResult(correlation=0.007144110205751199, pvalue=0.8579676052001091)\n",
      "formulaB_elements_IsFBlock\n",
      "PointbiserialrResult(correlation=-0.07012733484082277, pvalue=0.0786025895731158)\n",
      "formulaA_elements_IsMetal\n",
      "PointbiserialrResult(correlation=0.07571186356163453, pvalue=0.05752432375076138)\n",
      "formulaB_elements_IsMetal\n",
      "PointbiserialrResult(correlation=-0.006235904508716999, pvalue=0.8758669898674912)\n",
      "formulaA_elements_IsMetalloid\n",
      "PointbiserialrResult(correlation=-0.06565321642986129, pvalue=0.09968417954617662)\n",
      "formulaB_elements_IsMetalloid\n",
      "PointbiserialrResult(correlation=0.044870909342338484, pvalue=0.2607667628826585)\n",
      "formulaA_elements_IsNonmetal\n",
      "PointbiserialrResult(correlation=-0.057576970682670967, pvalue=0.1488801996996309)\n",
      "formulaB_elements_IsNonmetal\n",
      "PointbiserialrResult(correlation=-0.06748911655556894, pvalue=0.09054508540727958)\n",
      "formulaA_elements_MeltingT\n",
      "PointbiserialrResult(correlation=-0.05103263731943839, pvalue=0.2008270479799031)\n",
      "formulaB_elements_MeltingT\n",
      "PointbiserialrResult(correlation=0.03577884303467735, pvalue=0.3699615212672692)\n",
      "formulaA_elements_MendeleevNumber\n",
      "PointbiserialrResult(correlation=-0.18690637072738622, pvalue=2.3160997003257667e-06)\n",
      "formulaB_elements_MendeleevNumber\n",
      "PointbiserialrResult(correlation=0.028996823062814977, pvalue=0.46751891050424743)\n",
      "formulaA_elements_MiracleRadius\n",
      "PointbiserialrResult(correlation=0.18066156135769898, pvalue=5.0388566716925775e-06)\n",
      "formulaB_elements_MiracleRadius\n",
      "PointbiserialrResult(correlation=-0.06501812095216913, pvalue=0.10301140866689981)\n",
      "formulaA_elements_NUnfilled\n",
      "PointbiserialrResult(correlation=-0.02514379937854068, pvalue=0.5287268945541721)\n",
      "formulaB_elements_NUnfilled\n",
      "PointbiserialrResult(correlation=-0.09899695182471273, pvalue=0.012919584202706518)\n",
      "formulaA_elements_NValance\n",
      "PointbiserialrResult(correlation=-0.1253997684816354, pvalue=0.0016120612049272177)\n",
      "formulaB_elements_NValance\n",
      "PointbiserialrResult(correlation=-0.09638226073513705, pvalue=0.01552018928853375)\n",
      "formulaA_elements_NdUnfilled\n",
      "PointbiserialrResult(correlation=-0.038552793755669224, pvalue=0.3339916922470775)\n",
      "formulaB_elements_NdUnfilled\n",
      "PointbiserialrResult(correlation=-0.077620668490375, pvalue=0.05149483720026057)\n",
      "formulaA_elements_NdValence\n",
      "PointbiserialrResult(correlation=-0.13049850972073607, pvalue=0.001026973728091657)\n",
      "formulaB_elements_NdValence\n",
      "PointbiserialrResult(correlation=-0.05405370696395425, pvalue=0.17540683977546023)\n",
      "formulaA_elements_NfUnfilled\n",
      "PointbiserialrResult(correlation=0.027044560623401936, pvalue=0.49803235865201656)\n",
      "formulaB_elements_NfUnfilled\n",
      "PointbiserialrResult(correlation=-0.05572986749162497, pvalue=0.16237852657955484)\n",
      "formulaA_elements_NfValence\n",
      "PointbiserialrResult(correlation=-0.04012014769662357, pvalue=0.31469817385790216)\n",
      "formulaB_elements_NfValence\n",
      "PointbiserialrResult(correlation=-0.06567023013399469, pvalue=0.09959623603391171)\n",
      "formulaA_elements_NpUnfilled\n",
      "PointbiserialrResult(correlation=-0.09689419625337733, pvalue=0.01497752674585542)\n",
      "formulaB_elements_NpUnfilled\n",
      "PointbiserialrResult(correlation=-0.0024425389359910015, pvalue=0.9512114141503476)\n",
      "formulaA_elements_NpValence\n",
      "PointbiserialrResult(correlation=-0.09878653433739612, pvalue=0.013113632428550091)\n",
      "formulaB_elements_NpValence\n",
      "PointbiserialrResult(correlation=-0.12100001035619262, pvalue=0.002348328247576889)\n",
      "formulaA_elements_NsUnfilled\n",
      "PointbiserialrResult(correlation=0.09413284527506866, pvalue=0.018114321207838)\n",
      "formulaB_elements_NsUnfilled\n",
      "PointbiserialrResult(correlation=-0.06628768154983863, pvalue=0.09644621246078094)\n",
      "formulaA_elements_NsValence\n",
      "PointbiserialrResult(correlation=-0.07401743272844008, pvalue=0.06335659524206554)\n",
      "formulaB_elements_NsValence\n",
      "PointbiserialrResult(correlation=0.07030614827575557, pvalue=0.07784177002547293)\n",
      "formulaA_elements_Number\n",
      "PointbiserialrResult(correlation=-0.011368718042507837, pvalue=0.7758011504442677)\n",
      "formulaB_elements_Number\n",
      "PointbiserialrResult(correlation=-0.15211053710898856, pvalue=0.00012675157839360166)\n",
      "formulaA_elements_Polarizability\n",
      "PointbiserialrResult(correlation=0.22305817003457287, pvalue=1.522154746351239e-08)\n",
      "formulaB_elements_Polarizability\n",
      "PointbiserialrResult(correlation=-0.15176838925976502, pvalue=0.0001313231700595697)\n",
      "formulaA_elements_Row\n",
      "PointbiserialrResult(correlation=0.029307686949106074, pvalue=0.46275741433155226)\n",
      "formulaB_elements_Row\n",
      "PointbiserialrResult(correlation=-0.1843913920491551, pvalue=3.1774459575941407e-06)\n",
      "formulaA_elements_ShearModulus\n",
      "PointbiserialrResult(correlation=-0.03516420917644183, pvalue=0.3782451099688665)\n",
      "formulaB_elements_ShearModulus\n",
      "PointbiserialrResult(correlation=0.01972928713358683, pvalue=0.6211184423960097)\n",
      "formulaA_elements_SpaceGroupNumber\n",
      "PointbiserialrResult(correlation=0.07909974660169886, pvalue=0.0471939649467248)\n",
      "formulaB_elements_SpaceGroupNumber\n",
      "PointbiserialrResult(correlation=0.04755772467680861, pvalue=0.23326385367152186)\n",
      "avg_coordination_A\n",
      "PointbiserialrResult(correlation=0.02259366639104224, pvalue=0.5713648466862318)\n",
      "avg_coordination_B\n",
      "PointbiserialrResult(correlation=0.01717890098351183, pvalue=0.666932013589103)\n",
      "avg_nearest_neighbor_distance_A\n",
      "PointbiserialrResult(correlation=0.1811497742490977, pvalue=4.74624652217636e-06)\n",
      "avg_nearest_neighbor_distance_B\n",
      "PointbiserialrResult(correlation=-0.15632225550320564, pvalue=8.14431791328073e-05)\n"
     ]
    }
   ],
   "source": [
    "X_train_new=df_stable[feature_cols]\n",
    "y_new=y_stable\n",
    "correlation=[]\n",
    "for feature in feature_cols:\n",
    "    print(feature)\n",
    "    col_2_corr=X_train_new[feature]\n",
    "    \n",
    "    from scipy import stats\n",
    "    print(stats.pointbiserialr(col_2_corr, y_new))\n",
    "    [corr,p]=stats.pointbiserialr(col_2_corr, y_new)\n",
    "    correlation.append(corr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation=np.array(correlation)\n",
    "correlation=np.reshape(correlation,(1, 98))\n",
    "df_pb_corr_tmp= pd.DataFrame(correlation, columns = feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formulaA</th>\n",
       "      <th>formulaB</th>\n",
       "      <th>formulaA_elements_AtomicVolume</th>\n",
       "      <th>formulaB_elements_AtomicVolume</th>\n",
       "      <th>formulaA_elements_AtomicWeight</th>\n",
       "      <th>formulaB_elements_AtomicWeight</th>\n",
       "      <th>formulaA_elements_BoilingT</th>\n",
       "      <th>formulaB_elements_BoilingT</th>\n",
       "      <th>formulaA_elements_BulkModulus</th>\n",
       "      <th>formulaB_elements_BulkModulus</th>\n",
       "      <th>...</th>\n",
       "      <th>formulaA_elements_Row</th>\n",
       "      <th>formulaB_elements_Row</th>\n",
       "      <th>formulaA_elements_ShearModulus</th>\n",
       "      <th>formulaB_elements_ShearModulus</th>\n",
       "      <th>formulaA_elements_SpaceGroupNumber</th>\n",
       "      <th>formulaB_elements_SpaceGroupNumber</th>\n",
       "      <th>avg_coordination_A</th>\n",
       "      <th>avg_coordination_B</th>\n",
       "      <th>avg_nearest_neighbor_distance_A</th>\n",
       "      <th>avg_nearest_neighbor_distance_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.011369</td>\n",
       "      <td>-0.152111</td>\n",
       "      <td>0.204246</td>\n",
       "      <td>-0.188543</td>\n",
       "      <td>-0.011297</td>\n",
       "      <td>-0.14567</td>\n",
       "      <td>-0.048151</td>\n",
       "      <td>0.007295</td>\n",
       "      <td>-0.052503</td>\n",
       "      <td>0.094227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029308</td>\n",
       "      <td>-0.184391</td>\n",
       "      <td>-0.035164</td>\n",
       "      <td>0.019729</td>\n",
       "      <td>0.0791</td>\n",
       "      <td>0.047558</td>\n",
       "      <td>0.022594</td>\n",
       "      <td>0.017179</td>\n",
       "      <td>0.18115</td>\n",
       "      <td>-0.156322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   formulaA  formulaB  formulaA_elements_AtomicVolume  \\\n",
       "0 -0.011369 -0.152111                        0.204246   \n",
       "\n",
       "   formulaB_elements_AtomicVolume  formulaA_elements_AtomicWeight  \\\n",
       "0                       -0.188543                       -0.011297   \n",
       "\n",
       "   formulaB_elements_AtomicWeight  formulaA_elements_BoilingT  \\\n",
       "0                        -0.14567                   -0.048151   \n",
       "\n",
       "   formulaB_elements_BoilingT  formulaA_elements_BulkModulus  \\\n",
       "0                    0.007295                      -0.052503   \n",
       "\n",
       "   formulaB_elements_BulkModulus               ...                 \\\n",
       "0                       0.094227               ...                  \n",
       "\n",
       "   formulaA_elements_Row  formulaB_elements_Row  \\\n",
       "0               0.029308              -0.184391   \n",
       "\n",
       "   formulaA_elements_ShearModulus  formulaB_elements_ShearModulus  \\\n",
       "0                       -0.035164                        0.019729   \n",
       "\n",
       "   formulaA_elements_SpaceGroupNumber  formulaB_elements_SpaceGroupNumber  \\\n",
       "0                              0.0791                            0.047558   \n",
       "\n",
       "   avg_coordination_A  avg_coordination_B  avg_nearest_neighbor_distance_A  \\\n",
       "0            0.022594            0.017179                          0.18115   \n",
       "\n",
       "   avg_nearest_neighbor_distance_B  \n",
       "0                        -0.156322  \n",
       "\n",
       "[1 rows x 98 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pb_corr_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>formulaA</th>\n",
       "      <td>-0.011369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formulaB</th>\n",
       "      <td>-0.152111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formulaA_elements_AtomicVolume</th>\n",
       "      <td>0.204246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formulaB_elements_AtomicVolume</th>\n",
       "      <td>-0.188543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formulaA_elements_AtomicWeight</th>\n",
       "      <td>-0.011297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0\n",
       "formulaA                       -0.011369\n",
       "formulaB                       -0.152111\n",
       "formulaA_elements_AtomicVolume  0.204246\n",
       "formulaB_elements_AtomicVolume -0.188543\n",
       "formulaA_elements_AtomicWeight -0.011297"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pb_corr_tmp=df_pb_corr_tmp.transpose()\n",
    "df_pb_corr_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr=.1\n",
    "df_pb_corr_tmp=df_pb_corr_tmp[df_pb_corr_tmp.abs()>thr].dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pb_corr_tmp2=df_pb_corr_tmp.transpose()\n",
    "corr_variables_pbs=list(df_pb_corr_tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation has been calculated to build the model in the most relevant features ....\n",
      "Number of Results to train on: (630,)\n",
      "Number of Training Features before Pearson correlation: 98\n",
      "Pearson Correlation has identified 32 with  0.1\n",
      "Number of Training Features after Pearson correlation: 32\n",
      "(630, 32)\n",
      "(630, 32)\n",
      "(630, 32)\n",
      "(630, 32)\n",
      "(630, 20)\n",
      "(630, 20)\n",
      "Pearson Correlation in PCA Space has identified 13 with  0.05\n",
      "Number of Training Features after Pearson correlation in PCA Space: 13\n"
     ]
    }
   ],
   "source": [
    "# Pearson Correlation to Identify the features that influence the most on the output \n",
    "print('Pearson Correlation has been calculated to build the model in the most relevant features ....')\n",
    "X_train_new=df_stable[feature_cols] #This means we will only train on the elements that create a stable compound for this component of the stability vector and have at least one stable compound\n",
    "\n",
    "y_new=y_stable\n",
    "print('Number of Results to train on:',y_new.shape)\n",
    "print('Number of Training Features before Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "corr_df=pd.concat([X_train_new, y_new],axis=1)\n",
    "a=corr_df.corr()\n",
    "#a['Stable_compunds'].hist(bins=7, figsize=(18, 12), xlabelsize=10)\n",
    "\n",
    "## Incorporating the Features that contribute the most based on a pearson correlation coefficient threshold\n",
    "\n",
    "thr=.1\n",
    "\n",
    "corr_variables=list(a[a[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "del(corr_variables[-1])\n",
    "\n",
    "\n",
    "print('Pearson Correlation has identified', len(corr_variables), 'with ', str(thr) )\n",
    "\n",
    "## Normalization of Input Data\n",
    "\n",
    "## Using Un-normalized data as input\n",
    "X_train_new=df_stable[corr_variables]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "\n",
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "## Using Pearson Correlation in PCA\n",
    "df1= pd.DataFrame(data=X_train_new_PCA, index=indx_4_PC)\n",
    "print(df1.shape)\n",
    "\n",
    "corr_df_PCA=pd.concat([df1, y_new],axis=1)\n",
    "\n",
    "\n",
    "a_PCA=corr_df_PCA.corr()\n",
    "\n",
    "thr=.05\n",
    "corr_variables_PCA=list(a_PCA[a_PCA[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "\n",
    "del(corr_variables_PCA[-1])\n",
    "\n",
    "print('Pearson Correlation in PCA Space has identified', len(corr_variables_PCA), 'with ', str(thr) )\n",
    "\n",
    "X_train_PCA_PC=df1[corr_variables_PCA]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation in PCA Space:', X_train_PCA_PC.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_variables==corr_variables_pbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "formulaB                                       92.000000\n",
       "formulaA_elements_AtomicVolume                117.456016\n",
       "formulaB_elements_AtomicVolume                117.456016\n",
       "formulaB_elements_AtomicWeight                238.028910\n",
       "formulaA_elements_Column                       17.000000\n",
       "formulaA_elements_CovalentRadius              244.000000\n",
       "formulaB_elements_CovalentRadius              244.000000\n",
       "formulaB_elements_ElectronSurfaceDensityWS      1.850000\n",
       "formulaA_elements_Electronegativity             2.960000\n",
       "formulaA_elements_FirstIonizationEnergy        11.813800\n",
       "formulaA_elements_GSestBCClatcnt                6.140481\n",
       "formulaB_elements_GSestBCClatcnt                6.140481\n",
       "formulaA_elements_GSestFCClatcnt                7.736522\n",
       "formulaB_elements_GSestFCClatcnt                7.736522\n",
       "formulaA_elements_GSvolume_pa                 115.765000\n",
       "formulaB_elements_GSvolume_pa                 115.765000\n",
       "formulaB_elements_HeatCapacityMass              3.582000\n",
       "formulaB_elements_HeatCapacityMolar            75.690000\n",
       "formulaA_elements_ICSDVolume                   93.090000\n",
       "formulaB_elements_ICSDVolume                   93.090000\n",
       "formulaA_elements_IsAlkali                      1.000000\n",
       "formulaA_elements_MendeleevNumber              96.000000\n",
       "formulaA_elements_MiracleRadius               264.000000\n",
       "formulaA_elements_NValance                     29.000000\n",
       "formulaA_elements_NdValence                    10.000000\n",
       "formulaB_elements_NpValence                     5.000000\n",
       "formulaB_elements_Number                       92.000000\n",
       "formulaA_elements_Polarizability               59.420000\n",
       "formulaB_elements_Polarizability               59.420000\n",
       "formulaB_elements_Row                           7.000000\n",
       "avg_nearest_neighbor_distance_A                 5.323950\n",
       "avg_nearest_neighbor_distance_B                 5.323950\n",
       "dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new, y_new,\n",
    "                                                    test_size=.1,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test.mean())\n",
    "print(y_train.mean())\n",
    "print(y_new.mean())\n",
    "print(y_new.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting best Model\n",
    "print(' -- Optimal KNN --')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "rfc = KNeighborsClassifier(algorithm='auto',metric='minkowski',n_jobs=-1, n_neighbors=3, p=2,weights='uniform')\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print(' -- Defualt KNN --')\n",
    "\n",
    "rfc = KNeighborsClassifier()\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Grid\n",
    "print(' -- Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "\n",
    "grid_param = {  \n",
    "    'n_estimators': [100, 300, 500, 800, 1000],\n",
    "    'criterion': [ 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth':[1, 5, 10, 20],\n",
    "    'min_samples_split':[10,20 ,50 ,60 ,90 ,120],\n",
    "    'min_samples_leaf':[2 ,5 ,10, 25 ,50 ,90 ,120],\n",
    "    'min_impurity_decrease':[5e-7, 1e-6, 1e-5],\n",
    "}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=rfc,  \n",
    "                     param_grid=grid_param,\n",
    "                     scoring='accuracy',\n",
    "                     cv=10,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gd_sr.best_params_  \n",
    "print(best_parameters) \n",
    "best_result = gd_sr.best_score_  \n",
    "print(best_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fitting best Model\n",
    "print(' -- Optimal Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_opt = RandomForestClassifier(n_estimators=10,criterion='entropy',bootstrap=True,max_depth=100, \n",
    "                                 min_samples_split=2,\n",
    "                                 min_samples_leaf=3,\n",
    "                                 min_impurity_decrease=1e-5,\n",
    "                                 random_state=0\n",
    "                                 ,n_jobs=-1)\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "\n",
    "train_pred = rfc_opt.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)\n",
    "\n",
    "\n",
    "y_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "## Compare to Default Model\n",
    "print(' -- Default Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_def = RandomForestClassifier(class_weight={0:1-y_train.mean(), 1:y_train.mean()},n_jobs=-1,random_state=0)\n",
    "rfc_def.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "train_pred = rfc_def.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('DEF Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('DEF Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('DEF Training AUC:',roc_auc)\n",
    "\n",
    "y_pred = rfc_def.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = rfc_opt.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=10, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_train = rfc_opt.predict_proba(X_train)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores_train)\n",
    "freq_1=hist_1/y_scores_train.size\n",
    "    \n",
    "plt.hist(y_scores, bins=10, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rfc_opt.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(corr_variables, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, corr_variables, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "# Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "# Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of features for cumulative importance of 95%\n",
    "# Add 1 because Python is zero-indexed\n",
    "print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting and Bagging the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- ADABoosting Random Forest --')\n",
    "\n",
    "#first pass\n",
    "n_estimators = [1,3,5,10,50,100]\n",
    "criterion=['entropy','gini']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[2,5,10]\n",
    "\n",
    "min_samples_splits=[2,3,4,6,7,8,9,10,20]\n",
    "min_samples_leafs=[1,2,5,10]\n",
    "min_impurity_splits=[5e-7 ,1e-6]\n",
    "\n",
    "num_estimators=[1 ,10,100,300,500,700,1000]\n",
    "learning_reates=[.0001,.001,.01,.1,1,10]\n",
    "\n",
    "df_results_RF_Aboost=scores.hp_tune_ADAboosting_RF(X_train,y_train,X_test,y_test,10,n_estimators,criterion,bootstrap,max_depth,min_samples_splits,min_samples_leafs,min_impurity_splits,num_estimators,learning_reates)\n",
    "\n",
    "\n",
    "print('This are the best Parameters for ADABoosting with Random Forest:')\n",
    "print(df_results_RF_Aboost['features'][df_results_RF_Aboost['test_results_auc']==df_results_RF_Aboost['test_results_auc'].max()].head())\n",
    "\n",
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- ADABoosting Decision Trees --')\n",
    "\n",
    "\n",
    "criterion=['entropy','gini']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[1,2,5,10,100,250,1000]\n",
    "split=['random','best']\n",
    "min_samples_splits=[2,3,4,6,7,8,9,10]\n",
    "min_samples_leafs=[1]\n",
    "min_impurity_splits=[5e-7 ,1e-6]\n",
    "\n",
    "num_estimators=[1 ,10,100,300,500,700,1000]\n",
    "learning_reates=[.0001,.001,.01,.1,1,10]\n",
    "\n",
    "#second pass\n",
    "#criterion=['entropy']\n",
    "#max_depth=[10,11,15]\n",
    "#split=['random','best']\n",
    "#min_samples_splits=[2,3,4,6]\n",
    "#min_samples_leafs=[1,3,5]\n",
    "#min_impurity_splits=[3e-7, 5e-7,1e-6]\n",
    "\n",
    "#criterion=['entropy']\n",
    "#max_depth=[1,3,510]\n",
    "#split=['best']\n",
    "#min_samples_splits=[2,3]\n",
    "#min_samples_leafs=[1]\n",
    "#min_impurity_splits=[3e-7, 5e-7,8e-5]\n",
    "\n",
    "df_results_DT_ADA_boost=hp_tune_ADABoost_Decision_tree(X_train,y_train,X_test,y_test,10,criterion,max_depth,split,min_samples_splits,min_samples_leafs,min_impurity_splits,num_estimators,learning_reates)\n",
    "\n",
    "print('This are the best Parameters for ADABOOST Decision Tree:')\n",
    "print(df_results_DT_ADA_boost['features'][df_results_DT_ADA_boost['test_results_auc']==df_results_DT_ADA_boost['test_results_auc'].max()].head())\n",
    "\n",
    "\n",
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- Gradient boosting Decision Trees --')\n",
    "\n",
    "\n",
    "\n",
    "bootstrap= [True, False]\n",
    "max_depth=[1,2,5,10,100,250,1000]\n",
    "\n",
    "min_samples_splits=[2,3,4,6,7,8,9,10]\n",
    "min_samples_leafs=[1]\n",
    "min_impurity_splits=[5e-7 ,1e-6]\n",
    "\n",
    "num_estimators=[1 ,10,100,300,500,700,1000]\n",
    "learning_reates=[.0001,.001,.01,.1,1,10]\n",
    "\n",
    "#second pass\n",
    "#criterion=['entropy']\n",
    "#max_depth=[10,11,15]\n",
    "#split=['random','best']\n",
    "#min_samples_splits=[2,3,4,6]\n",
    "#min_samples_leafs=[1,3,5]\n",
    "#min_impurity_splits=[3e-7, 5e-7,1e-6]\n",
    "\n",
    "#criterion=['entropy']\n",
    "#max_depth=[1,3,510]\n",
    "#split=['best']\n",
    "#min_samples_splits=[2,3]\n",
    "#min_samples_leafs=[1]\n",
    "#min_impurity_splits=[3e-7, 5e-7,8e-5]\n",
    "\n",
    "df_results_DT_GRAD_boost=hp_tune_GRADBoost_Decision_tree(X_train,y_train,X_test,y_test,10,max_depth,min_samples_splits,min_samples_leafs,min_impurity_splits,num_estimators,learning_reates)\n",
    "print('This are the best Parameters for GRAD BOOST Decision Tree:')\n",
    "print(df_results_DT_GRAD_boost['features'][df_results_DT_GRAD_boost['test_results_auc']==df_results_DT_GRAD_boost['test_results_auc'].max()].head())\n",
    "\n",
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- hp_tune_Extra_trees --')\n",
    "\n",
    "#first pass\n",
    "n_estimators = [1,3,5,10,50,100]\n",
    "criterion=['entropy','gini']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[2,5,10]\n",
    "\n",
    "min_samples_splits=[2,3,4,6,7,8,9,10,20]\n",
    "min_samples_leafs=[1,2,5,10]\n",
    "min_impurity_splits=[5e-7 ,1e-6]\n",
    "\n",
    "#second pass\n",
    "#n_estimators = [10,20,50]\n",
    "#criterion=['entropy']\n",
    "#bootstrap= [True, False]\n",
    "#max_depth=[5,6]\n",
    "#min_samples_splits=[2,3,4,5,6]\n",
    "#min_samples_leafs=[1,3,5]\n",
    "#min_impurity_splits=[3e-7, 5e-7,1e-6]\n",
    "\n",
    "#n_estimators = [1,3,5,8]\n",
    "#criterion=['entropy']\n",
    "#bootstrap= [True, False]\n",
    "#max_depth=[1,3,4]\n",
    "\n",
    "\n",
    "#min_samples_splits=[2,3,4,5]\n",
    "#min_samples_leafs=[1]\n",
    "#min_impurity_splits=[3e-7, 5e-7,8e-7]\n",
    "\n",
    "df_results_extra_trees=scores.hp_tune_Random_Forest(X_train,y_train,X_test,y_test,10,n_estimators,criterion,bootstrap,max_depth,min_samples_splits,min_samples_leafs,min_impurity_splits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('This are the best Parameters for Random Forest:')\n",
    "print(df_results_extra_trees['features'][df_results_extra_trees['test_results_auc']==df_results_extra_trees['test_results_auc'].max()].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- Random Forest --')\n",
    "\n",
    "#first pass\n",
    "n_estimators = [1,3,5,10,50,100]\n",
    "criterion=['entropy','gini']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[2,5,10]\n",
    "\n",
    "min_samples_splits=[2,3,4,6,7,8,9,10,20]\n",
    "min_samples_leafs=[1,2,5,10]\n",
    "min_impurity_splits=[5e-7 ,1e-6]\n",
    "\n",
    "num_estimators=[1 ,10,100,300,500,700,1000]\n",
    "learning_reates=[.0001,.001,.01,.1,1,10]\n",
    "\n",
    "df_results_RF_Aboost=scores.hp_tune_ADAboosting_RF(X_train,y_train,X_test,y_test,10,n_estimators,criterion,bootstrap,max_depth,min_samples_splits,min_samples_leafs,min_impurity_splits,num_estimators,learning_reates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print('------- Extra Trees Classifier-------')\n",
    "clf_ex = ExtraTreesClassifier(class_weight={0:1-y_train.mean(), 1:y_train.mean()},min_samples_split=5,min_samples_leaf=2,min_impurity_decrease=1e-7)\n",
    "\n",
    "\n",
    "clf_ex.fit(X_train, y_train)\n",
    "\n",
    "train_pred = rfc_opt.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)\n",
    "\n",
    "y_pred = clf_ex.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf_ex.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .1)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = clf_ex.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=100, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADAboosting\n",
    "print('------ ADAboosting with Random Forest ----')\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=clf_ex, n_estimators=10,learning_rate=.01)\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf,X=X_train, y=y_train, cv=10)\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .1)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=100, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient boosting\n",
    "print('------ Gradient Boosting with Decision Trees ----')\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf  = GradientBoostingClassifier(n_estimators=60, learning_rate=.1,min_samples_split=50,min_samples_leaf=50)\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf,X=X_train, y=y_train, cv=10,scoring='roc_auc')\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .25)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=100, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[np.logical_not(y_pred==y_test.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[1769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new[X_train_new.index==1769]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' -- Optimal Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "rfc_opt = RandomForestClassifier()\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=rfc_opt, X=X_train, y=y_train, cv=10)\n",
    "\n",
    "\n",
    "all_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_opt.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rfc_opt.predict(X_train)\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,y_train_pred)\n",
    "print('Training  precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "def precision_recall_threshold(p, r, thresholds, t=0.5):\n",
    "    \"\"\"\n",
    "    plots the precision recall curve and shows the current value for each\n",
    "    by identifying the classifier's threshold (t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate new class predictions based on the adjusted_classes\n",
    "    # function above and view the resulting confusion matrix.\n",
    "    y_pred_adj = adjusted_classes(y_scores, t)\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),\n",
    "                       columns=['pred_neg', 'pred_pos'], \n",
    "                       index=['neg', 'pos']))\n",
    "    \n",
    "    # plot the curve\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Precision and Recall curve ^ = current threshold\")\n",
    "    plt.step(r, p, color='r', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(r, p, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    plt.ylim([0.5, 1.01]);\n",
    "    plt.xlim([0.5, 1.01]);\n",
    "    plt.xlabel('Recall');\n",
    "    plt.ylabel('Precision');\n",
    "    \n",
    "    # plot the current threshold on the line\n",
    "    close_default_clf = np.argmin(np.abs(thresholds - t))\n",
    "    plt.plot(r[close_default_clf], p[close_default_clf], '^', c='k',\n",
    "            markersize=15)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,precision_recall_curve\n",
    "p, r, thresholds = precision_recall_curve(y_test, y_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.precision_recall_threshold(p, r, thresholds, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_vs_threshold(p, r, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, auc_thresholds = roc_curve(y_test, y_scores)\n",
    "print(auc(fpr, tpr)) # AUC of ROC\n",
    "plot_roc_curve(fpr, tpr, 'recall_optimized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- Random Forest --')\n",
    "\n",
    "n_estimators = [50,100,200]\n",
    "criterion=['gini', 'entropy']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[10, 20,30]\n",
    "\n",
    "\n",
    "df_results_RF=scores.hp_tune_Random_Forest(X_train,y_train,X_test,y_test,10,n_estimators,criterion,bootstrap,max_depth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('This are the best Parameters for Random Forest:')\n",
    "print(df_results_RF[df_results_RF['test_recall']==df_results_RF['test_recall'].max()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_RF[['test_recall','test_accuracy','test_precision','features']][df_results_RF['test_precision']==df_results_RF['test_precision'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualization Aid for identifying the best elements to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train[stab_vec_list]\n",
    "print(y.sum(axis=1).value_counts())\n",
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "f,a = plt.subplots(3,3)\n",
    "f.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "a = a.ravel()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "for count,ax in enumerate(a):\n",
    "    \n",
    "    y = df_train[stab_vec_list[count]]\n",
    "    #print(y.value_counts())\n",
    "    hist_1, bin_edges_1 = np.histogram(y)\n",
    "    freq_1=hist_1/y.size\n",
    "    \n",
    "    ax.hist(y.values, bins=10, label='all elements')\n",
    "\n",
    "\n",
    "    #ax.xlim(min(bin_edges), max(bin_edges))\n",
    "    #ax.title(stab_vec_list[count])\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "    \n",
    "\n",
    "#for count in range(9):\n",
    "\n",
    "    #y = df_train[stab_vec_list[count]]\n",
    "    stable_comp=df_train.loc[y==1,['formulaA','formulaB']]\n",
    "    #print('Compound being analyzed is',stab_vec_list[count])\n",
    "    stable_comp_num=stable_comp.values\n",
    "    stable_A=np.unique(stable_comp_num[:,0])\n",
    "    stable_B=np.unique(stable_comp_num[:,1])\n",
    "    df_unique= pd.DataFrame()\n",
    "    #print(df_unique.shape)\n",
    "\n",
    "    y_unique= pd.DataFrame()\n",
    "    \n",
    "    for cnt in range(stable_A.shape[0]):\n",
    "        #print(stable_A[cnt])\n",
    "        df_tmp=y.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp],axis=0)\n",
    "        #print(df_tmp.shape)\n",
    "        #print(df_unique.shape)\n",
    "    \n",
    "    #print(y_unique.shape)\n",
    "\n",
    "    for cnt in range(stable_B.shape[0]):\n",
    "        #print(stable_A[cnt])\n",
    "        df_tmp=y.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp],axis=0)\n",
    "\n",
    "    \n",
    "    y_unique=y.iloc[y_unique.index.unique()]\n",
    "    ax.hist(y_unique.values, bins=10, label='stable elements')\n",
    "    #print(y_unique.value_counts())\n",
    "\n",
    "    #ax.xlim(min(bin_edges), max(bin_edges))\n",
    "    #ax.title()\n",
    "    #print(stab_vec_list[count])\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "    \n",
    "    y_stable=y_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "    ax.hist(y_stable.values, bins=10, label='stable elements')\n",
    "    #print(y_stable.value_counts())\n",
    "\n",
    "    #ax.xlim(min(bin_edges), max(bin_edges))\n",
    "    #ax.title()\n",
    "    #print(stab_vec_list[count])\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
