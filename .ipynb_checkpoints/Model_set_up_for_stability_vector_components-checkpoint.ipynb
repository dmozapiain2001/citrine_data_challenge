{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data is being read ....\n",
      "(2572, 98)\n",
      "(2572, 109)\n",
      "Training Data has been read and feature engineering is being performed....\n",
      "(2572, 110)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formulaA</th>\n",
       "      <th>formulaB</th>\n",
       "      <th>formulaA_elements_AtomicVolume</th>\n",
       "      <th>formulaB_elements_AtomicVolume</th>\n",
       "      <th>formulaA_elements_AtomicWeight</th>\n",
       "      <th>formulaB_elements_AtomicWeight</th>\n",
       "      <th>formulaA_elements_BoilingT</th>\n",
       "      <th>formulaB_elements_BoilingT</th>\n",
       "      <th>formulaA_elements_BulkModulus</th>\n",
       "      <th>formulaB_elements_BulkModulus</th>\n",
       "      <th>...</th>\n",
       "      <th>A82B</th>\n",
       "      <th>A73B</th>\n",
       "      <th>A64B</th>\n",
       "      <th>A55B</th>\n",
       "      <th>A46B</th>\n",
       "      <th>A37B</th>\n",
       "      <th>A28B</th>\n",
       "      <th>A19B</th>\n",
       "      <th>B</th>\n",
       "      <th>Stable_compunds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>17.075648</td>\n",
       "      <td>227.0</td>\n",
       "      <td>107.868200</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>71</td>\n",
       "      <td>37</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>16.594425</td>\n",
       "      <td>227.0</td>\n",
       "      <td>26.981539</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>32</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>21.723966</td>\n",
       "      <td>227.0</td>\n",
       "      <td>74.921600</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>20</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>64.969282</td>\n",
       "      <td>227.0</td>\n",
       "      <td>137.327000</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71</td>\n",
       "      <td>52</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>35.483459</td>\n",
       "      <td>227.0</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   formulaA  formulaB  formulaA_elements_AtomicVolume  \\\n",
       "0        71         3                       37.433086   \n",
       "1        71        37                       37.433086   \n",
       "2        71        32                       37.433086   \n",
       "3        71        20                       37.433086   \n",
       "4        71        52                       37.433086   \n",
       "\n",
       "   formulaB_elements_AtomicVolume  formulaA_elements_AtomicWeight  \\\n",
       "0                       17.075648                           227.0   \n",
       "1                       16.594425                           227.0   \n",
       "2                       21.723966                           227.0   \n",
       "3                       64.969282                           227.0   \n",
       "4                       35.483459                           227.0   \n",
       "\n",
       "   formulaB_elements_AtomicWeight  formulaA_elements_BoilingT  \\\n",
       "0                      107.868200                      3473.0   \n",
       "1                       26.981539                      3473.0   \n",
       "2                       74.921600                      3473.0   \n",
       "3                      137.327000                      3473.0   \n",
       "4                      208.980400                      3473.0   \n",
       "\n",
       "   formulaB_elements_BoilingT  formulaA_elements_BulkModulus  \\\n",
       "0                      2435.0                            0.0   \n",
       "1                      2792.0                            0.0   \n",
       "2                       887.0                            0.0   \n",
       "3                      2143.0                            0.0   \n",
       "4                      1837.0                            0.0   \n",
       "\n",
       "   formulaB_elements_BulkModulus       ...         A82B  A73B  A64B  A55B  \\\n",
       "0                          100.0       ...            0     1     0     1   \n",
       "1                           76.0       ...            0     1     0     0   \n",
       "2                           22.0       ...            0     0     0     0   \n",
       "3                            9.6       ...            0     0     0     0   \n",
       "4                           31.0       ...            0     0     0     0   \n",
       "\n",
       "   A46B  A37B  A28B  A19B  B  Stable_compunds  \n",
       "0     0     0     0     0  1                1  \n",
       "1     0     0     0     0  1                1  \n",
       "2     0     0     0     0  1                0  \n",
       "3     0     0     0     0  1                0  \n",
       "4     0     0     0     0  1                0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# # Reading in the Data\n",
    "\n",
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "        \n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "df_train.shape\n",
    "\n",
    "\n",
    "# ## Data Manipulation\n",
    "print('Training Data is being read ....')\n",
    "#  - Transforming the outcome to a numpy vector\n",
    "\n",
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "csvfile = csv.reader(open(path_train,'r'))\n",
    "header = next(csvfile)\n",
    "\n",
    "formulaA = []\n",
    "formulaB = []\n",
    "\n",
    "for row in csvfile:\n",
    "    formulaA.append(row[0])\n",
    "    formulaB.append(row[1])\n",
    "formulas = formulaA + formulaB\n",
    "formulas = list(set(formulas))\n",
    "\n",
    "# -- /!\\ need to save the dict as the ordering may difer at each run\n",
    "formula2int = {}\n",
    "int2formula = {}\n",
    "for i, f in enumerate(formulas):\n",
    "    formula2int[f] = i\n",
    "    int2formula[i] = f\n",
    "\n",
    "formulaAint = np.array([formula2int[x] for x in formulaA])\n",
    "formulaBint = np.array([formula2int[x] for x in formulaB])\n",
    "\n",
    "df_train['formulaA']=formulaAint\n",
    "df_train['formulaB']=formulaBint\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "# ### Input Data Normalization and Feature Engineering\n",
    "print('Training Data has been read and feature engineering is being performed....')\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_data.csv', 'test_data.csv']\n"
     ]
    }
   ],
   "source": [
    "print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now getting the Output for each Vector Component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2522\n",
      "1      50\n",
      "Name: A91B, dtype: int64\n",
      "Compound being analyzed is A91B\n",
      "0    1332\n",
      "1      50\n",
      "Name: A91B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector: (1382,)\n",
      "0    580\n",
      "1     50\n",
      "Name: A91B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound: (630,)\n"
     ]
    }
   ],
   "source": [
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "for count in range(0,1):\n",
    "    \n",
    "    y = df_train[stab_vec_list[count]]\n",
    "    print(y.value_counts())\n",
    "\n",
    "    stable_comp=df_train.loc[y==1,['formulaA','formulaB']] # Find the elements that create a stable element in this vector component\n",
    "    print('Compound being analyzed is',stab_vec_list[count])\n",
    "    stable_comp_num=stable_comp.values\n",
    "    stable_A=np.unique(stable_comp_num[:,0])\n",
    "    stable_B=np.unique(stable_comp_num[:,1])\n",
    "    \n",
    "    df_unique= pd.DataFrame()\n",
    "\n",
    "    y_unique= pd.DataFrame()\n",
    "    \n",
    "    for cnt in range(stable_A.shape[0]):\n",
    "\n",
    "        df_tmp1=y.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "        df_tmp=df_train.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "        df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    for cnt in range(stable_B.shape[0]):\n",
    "        df_tmp1=y.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "        df_tmp=df_train.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "        df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "\n",
    "    \n",
    "    y_unique=y.iloc[y_unique.index.unique()]\n",
    "    df_unique=df_train.iloc[df_unique.index.unique()]\n",
    "    print(y_unique.value_counts())\n",
    "    print('The elements in these compounds create a stable compound for this component of the stability vector:',y_unique.shape)\n",
    "    \n",
    "    \n",
    "    y_stable=y_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "    df_stable=df_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "    print(y_stable.value_counts())\n",
    "    print('The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound:',y_stable.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation has been calculated to build the model in the most relevant features ....\n",
      "Number of Results to train on: (630,)\n",
      "Number of Training Features before Pearson correlation: 98\n",
      "Pearson Correlation has identified 32 with  0.1\n",
      "Number of Training Features after Pearson correlation: 32\n",
      "(630, 32)\n",
      "(630, 32)\n",
      "(630, 32)\n",
      "(630, 32)\n",
      "(630, 20)\n",
      "(630, 20)\n",
      "Pearson Correlation in PCA Space has identified 14 with  0.05\n",
      "Number of Training Features after Pearson correlation in PCA Space: 14\n"
     ]
    }
   ],
   "source": [
    "# Pearson Correlation to Identify the features that influence the most on the output \n",
    "print('Pearson Correlation has been calculated to build the model in the most relevant features ....')\n",
    "X_train_new=df_stable[feature_cols] #This means we will only train on the elements that create a stable compound for this component of the stability vector and have at least one stable compound\n",
    "\n",
    "y_new=y_stable\n",
    "print('Number of Results to train on:',y_new.shape)\n",
    "print('Number of Training Features before Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "corr_df=pd.concat([X_train_new, y_new],axis=1)\n",
    "a=corr_df.corr()\n",
    "#a['Stable_compunds'].hist(bins=7, figsize=(18, 12), xlabelsize=10)\n",
    "\n",
    "## Incorporating the Features that contribute the most based on a pearson correlation coefficient threshold\n",
    "\n",
    "thr=.1\n",
    "\n",
    "corr_variables=list(a[a[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "del(corr_variables[-1])\n",
    "\n",
    "\n",
    "print('Pearson Correlation has identified', len(corr_variables), 'with ', str(thr) )\n",
    "\n",
    "## Normalization of Input Data\n",
    "\n",
    "## Using Un-normalized data as input\n",
    "X_train_new=df_stable[corr_variables]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "\n",
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "## Using Pearson Correlation in PCA\n",
    "df1= pd.DataFrame(data=X_train_new_PCA, index=indx_4_PC)\n",
    "print(df1.shape)\n",
    "\n",
    "corr_df_PCA=pd.concat([df1, y_new],axis=1)\n",
    "\n",
    "\n",
    "a_PCA=corr_df_PCA.corr()\n",
    "\n",
    "thr=.05\n",
    "corr_variables_PCA=list(a_PCA[a_PCA[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "\n",
    "del(corr_variables_PCA[-1])\n",
    "\n",
    "print('Pearson Correlation in PCA Space has identified', len(corr_variables_PCA), 'with ', str(thr) )\n",
    "\n",
    "X_train_PCA_PC=df1[corr_variables_PCA]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation in PCA Space:', X_train_PCA_PC.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(567, 32) (567,)\n",
      "(63, 32) (63,)\n"
     ]
    }
   ],
   "source": [
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new_m1_p1, y_new,\n",
    "                                                    test_size=.1,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06349206349206349\n",
      "0.08112874779541446\n",
      "0.07936507936507936\n",
      "0    580\n",
      "1     50\n",
      "Name: A91B, dtype: int64\n",
      "0    59\n",
      "1     4\n",
      "Name: A91B, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_test.mean())\n",
    "print(y_train.mean())\n",
    "print(y_new.mean())\n",
    "print(y_new.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Grid\n",
    "print(' -- Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV  \n",
    "\n",
    "grid_param = {  \n",
    "    'n_estimators': [100, 300, 500, 800, 1000],\n",
    "    'criterion': [ 'entropy'],\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth':[1, 5, 10, 20],\n",
    "    'min_samples_split':[10,20 ,50 ,60 ,90 ,120],\n",
    "    'min_samples_leaf':[2 ,5 ,10, 25 ,50 ,90 ,120],\n",
    "    'min_impurity_decrease':[5e-7, 1e-6, 1e-5],\n",
    "}\n",
    "\n",
    "gd_sr = GridSearchCV(estimator=rfc,  \n",
    "                     param_grid=grid_param,\n",
    "                     scoring='accuracy',\n",
    "                     cv=10,\n",
    "                     n_jobs=-1)\n",
    "\n",
    "gd_sr.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = gd_sr.best_params_  \n",
    "print(best_parameters) \n",
    "best_result = gd_sr.best_score_  \n",
    "print(best_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Optimal Random Forest --\n",
      "Training precision:  1.0   recall:  0.08695652173913043   F1:  0.16   accuracy:  0.9259259259259259\n",
      "Training Confusion matrix\n",
      "[[521   0]\n",
      " [ 42   4]]\n",
      "Training AUC: 0.5434782608695652\n",
      "Optimal precision:  1.0   recall:  0.25   F1:  0.4   accuracy:  0.9523809523809523\n",
      "optimal Confusion matrix\n",
      "[[59  0]\n",
      " [ 3  1]]\n",
      "Optimal AUC: 0.625\n",
      " -- Default Random Forest --\n",
      "DEF Training precision:  1.0   recall:  0.9347826086956522   F1:  0.9662921348314606   accuracy:  0.9947089947089947\n",
      "DEF Training Confusion matrix\n",
      "[[521   0]\n",
      " [  3  43]]\n",
      "DEF Training AUC: 0.9673913043478262\n",
      "Defualt Model precision:  0.5   recall:  0.25   F1:  0.3333333333333333   accuracy:  0.9365079365079365\n",
      "Defualt ModelConfusion matrix\n",
      "[[58  1]\n",
      " [ 3  1]]\n",
      "Defualt ModelAUC: 0.6165254237288136\n"
     ]
    }
   ],
   "source": [
    "## Fitting best Model\n",
    "print(' -- Optimal Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_opt = RandomForestClassifier(n_estimators=100,criterion='entropy',bootstrap=True,max_depth=100, \n",
    "                                 class_weight={0:1-y_train.mean(), 1:y_train.mean()},\n",
    "                                 min_samples_split=2,\n",
    "                                 min_samples_leaf=4,\n",
    "                                 min_impurity_decrease=5e-7,\n",
    "                                 random_state=0\n",
    "                                 ,n_jobs=-1)\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "\n",
    "train_pred = rfc_opt.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)\n",
    "\n",
    "\n",
    "y_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "## Compare to Default Model\n",
    "print(' -- Default Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc_def = RandomForestClassifier(class_weight={0:1-y_train.mean(), 1:y_train.mean()},n_jobs=-1,random_state=0)\n",
    "rfc_def.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "train_pred = rfc_def.predict(X_train)\n",
    "    \n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,train_pred)\n",
    "print('DEF Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('DEF Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('DEF Training AUC:',roc_auc)\n",
    "\n",
    "y_pred = rfc_def.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = rfc_opt.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=10, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores_train = rfc_opt.predict_proba(X_train)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores_train)\n",
    "freq_1=hist_1/y_scores_train.size\n",
    "    \n",
    "plt.hist(y_scores, bins=10, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rfc_opt.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(corr_variables, importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, corr_variables, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "# Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "# Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of features for cumulative importance of 95%\n",
    "# Add 1 because Python is zero-indexed\n",
    "print('Number of features for 95% importance:', np.where(cumulative_importances > 0.95)[0][0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting and Bagging the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print('------- Extra Trees Classifier-------')\n",
    "clf_ex = ExtraTreesClassifier(class_weight={0:1-y_train.mean(), 1:y_train.mean()})\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf_ex, X=X_train, y=y_train, cv=10)\n",
    "\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf_ex.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_ex.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf_ex.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .1)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = clf_ex.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=100, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ADAboosting\n",
    "print('------ ADAboosting with Random Forest ----')\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=clf_ex, n_estimators=500,learning_rate=.01)\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf,X=X_train, y=y_train, cv=10)\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .1)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=100, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gradient boosting\n",
    "print('------ Gradient Boosting with Decision Trees ----')\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf  = GradientBoostingClassifier(n_estimators=1000, learning_rate=.1)\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=clf,X=X_train, y=y_train, cv=10)\n",
    "\n",
    "print(all_accuracies.mean())\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Defualt Model precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Defualt ModelConfusion matrix')\n",
    "print(confusion)\n",
    "print('Defualt ModelAUC:',roc_auc)\n",
    "\n",
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y_pred_adj = scores.adjusted_classes(y_scores, .0001)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred_adj)\n",
    "print('Adjusted Threshold precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Adjusted Threshold Confusion matrix')\n",
    "print(confusion)\n",
    "print('Adjusted Threshold AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = clf.predict_proba(X_test)[:, 1]\n",
    "hist_1, bin_edges_1 = np.histogram(y_scores)\n",
    "freq_1=hist_1/y_scores.size\n",
    "    \n",
    "plt.hist(y_scores, bins=100, label='all elements')\n",
    "\n",
    "\n",
    "plt.xlim(min(bin_edges_1), max(bin_edges_1))\n",
    "plt.title(stab_vec_list[count])\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[np.logical_not(y_pred==y_test.values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.iloc[1769]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new[X_train_new.index==1769]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' -- Optimal Random Forest --')\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score \n",
    "rfc_opt = RandomForestClassifier()\n",
    "\n",
    "all_accuracies = cross_val_score(estimator=rfc_opt, X=X_train, y=y_train, cv=10)\n",
    "\n",
    "\n",
    "all_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_opt.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = rfc_opt.predict(X_train)\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_train,y_train_pred)\n",
    "print('Training  precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\n",
    "\n",
    "def adjusted_classes(y_scores, t):\n",
    "    \"\"\"\n",
    "    This function adjusts class predictions based on the prediction threshold (t).\n",
    "    Will only work for binary classification problems.\n",
    "    \"\"\"\n",
    "    return [1 if y >= t else 0 for y in y_scores]\n",
    "\n",
    "def precision_recall_threshold(p, r, thresholds, t=0.5):\n",
    "    \"\"\"\n",
    "    plots the precision recall curve and shows the current value for each\n",
    "    by identifying the classifier's threshold (t).\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate new class predictions based on the adjusted_classes\n",
    "    # function above and view the resulting confusion matrix.\n",
    "    y_pred_adj = adjusted_classes(y_scores, t)\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),\n",
    "                       columns=['pred_neg', 'pred_pos'], \n",
    "                       index=['neg', 'pos']))\n",
    "    \n",
    "    # plot the curve\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title(\"Precision and Recall curve ^ = current threshold\")\n",
    "    plt.step(r, p, color='r', alpha=0.2,\n",
    "             where='post')\n",
    "    plt.fill_between(r, p, step='post', alpha=0.2,\n",
    "                     color='b')\n",
    "    plt.ylim([0.5, 1.01]);\n",
    "    plt.xlim([0.5, 1.01]);\n",
    "    plt.xlabel('Recall');\n",
    "    plt.ylabel('Precision');\n",
    "    \n",
    "    # plot the current threshold on the line\n",
    "    close_default_clf = np.argmin(np.abs(thresholds - t))\n",
    "    plt.plot(r[close_default_clf], p[close_default_clf], '^', c='k',\n",
    "            markersize=15)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,precision_recall_curve\n",
    "p, r, thresholds = precision_recall_curve(y_test, y_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.precision_recall_threshold(p, r, thresholds, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    \"\"\"\n",
    "    Modified from:\n",
    "    Hands-On Machine learning with Scikit-Learn\n",
    "    and TensorFlow; p.89\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(\"Precision and Recall Scores as a function of the decision threshold\")\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Decision Threshold\")\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_vs_threshold(p, r, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    \"\"\"\n",
    "    The ROC curve, modified from \n",
    "    Hands-On Machine learning with Scikit-Learn and TensorFlow; p.91\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([-0.005, 1, 0, 1.005])\n",
    "    plt.xticks(np.arange(0,1, 0.05), rotation=90)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate (Recall)\")\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, auc_thresholds = roc_curve(y_test, y_scores)\n",
    "print(auc(fpr, tpr)) # AUC of ROC\n",
    "plot_roc_curve(fpr, tpr, 'recall_optimized')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-Parameter Search Grid Using 10-Fold CV and Test\n",
    "print(' -- Random Forest --')\n",
    "\n",
    "n_estimators = [50,100,200]\n",
    "criterion=['gini', 'entropy']\n",
    "bootstrap= [True, False]\n",
    "max_depth=[10, 20,30]\n",
    "\n",
    "\n",
    "df_results_RF=scores.hp_tune_Random_Forest(X_train,y_train,X_test,y_test,10,n_estimators,criterion,bootstrap,max_depth)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('This are the best Parameters for Random Forest:')\n",
    "print(df_results_RF[df_results_RF['test_recall']==df_results_RF['test_recall'].max()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_RF[['test_recall','test_accuracy','test_precision','features']][df_results_RF['test_precision']==df_results_RF['test_precision'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vizualization Aid for identifying the best elements to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train[stab_vec_list]\n",
    "print(y.sum(axis=1).value_counts())\n",
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "f,a = plt.subplots(3,3)\n",
    "f.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "a = a.ravel()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "for count,ax in enumerate(a):\n",
    "    \n",
    "    y = df_train[stab_vec_list[count]]\n",
    "    #print(y.value_counts())\n",
    "    hist_1, bin_edges_1 = np.histogram(y)\n",
    "    freq_1=hist_1/y.size\n",
    "    \n",
    "    ax.hist(y.values, bins=10, label='all elements')\n",
    "\n",
    "\n",
    "    #ax.xlim(min(bin_edges), max(bin_edges))\n",
    "    #ax.title(stab_vec_list[count])\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "    \n",
    "\n",
    "#for count in range(9):\n",
    "\n",
    "    #y = df_train[stab_vec_list[count]]\n",
    "    stable_comp=df_train.loc[y==1,['formulaA','formulaB']]\n",
    "    #print('Compound being analyzed is',stab_vec_list[count])\n",
    "    stable_comp_num=stable_comp.values\n",
    "    stable_A=np.unique(stable_comp_num[:,0])\n",
    "    stable_B=np.unique(stable_comp_num[:,1])\n",
    "    df_unique= pd.DataFrame()\n",
    "    #print(df_unique.shape)\n",
    "\n",
    "    y_unique= pd.DataFrame()\n",
    "    \n",
    "    for cnt in range(stable_A.shape[0]):\n",
    "        #print(stable_A[cnt])\n",
    "        df_tmp=y.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp],axis=0)\n",
    "        #print(df_tmp.shape)\n",
    "        #print(df_unique.shape)\n",
    "    \n",
    "    #print(y_unique.shape)\n",
    "\n",
    "    for cnt in range(stable_B.shape[0]):\n",
    "        #print(stable_A[cnt])\n",
    "        df_tmp=y.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "        y_unique=pd.concat([y_unique, df_tmp],axis=0)\n",
    "\n",
    "    \n",
    "    y_unique=y.iloc[y_unique.index.unique()]\n",
    "    ax.hist(y_unique.values, bins=10, label='stable elements')\n",
    "    #print(y_unique.value_counts())\n",
    "\n",
    "    #ax.xlim(min(bin_edges), max(bin_edges))\n",
    "    #ax.title()\n",
    "    #print(stab_vec_list[count])\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "    \n",
    "    y_stable=y_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "    ax.hist(y_stable.values, bins=10, label='stable elements')\n",
    "    #print(y_stable.value_counts())\n",
    "\n",
    "    #ax.xlim(min(bin_edges), max(bin_edges))\n",
    "    #ax.title()\n",
    "    #print(stab_vec_list[count])\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_xlabel('Value')\n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "    \n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
