{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data is being read ....\n",
      "(2572, 98)\n",
      "(2572, 109)\n",
      "Training Data has been read and feature engineering is being performed....\n",
      "(2572, 110)\n",
      "['training_data.csv', 'test_data.csv']\n",
      "0    2484\n",
      "1      88\n",
      "Name: A82B, dtype: int64\n",
      "Compound being analyzed is A82B\n",
      "0    1702\n",
      "1      88\n",
      "Name: A82B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector: (1790,)\n",
      "0    813\n",
      "1     88\n",
      "Name: A82B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound: (901,)\n",
      "Pearson Correlation has been calculated to build the model in the most relevant features ....\n",
      "Number of Results to train on: (901,)\n",
      "Number of Training Features before Pearson correlation: 98\n",
      "Pearson Correlation has identified 30 with  0.09\n",
      "Number of Training Features after Pearson correlation: 30\n",
      "(901, 30)\n",
      "(901, 30)\n",
      "(901, 30)\n",
      "(901, 30)\n",
      "(901, 20)\n",
      "(901, 20)\n",
      "Pearson Correlation in PCA Space has identified 7 with  0.05\n",
      "Number of Training Features after Pearson correlation in PCA Space: 7\n",
      "Training Model Using Z-normalized Data\n",
      "(765, 30) (765,)\n",
      "(136, 30) (136,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scores\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "\n",
    "# # Reading in the Data\n",
    "\n",
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "        \n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "df_train.shape\n",
    "\n",
    "\n",
    "# ## Data Manipulation\n",
    "print('Training Data is being read ....')\n",
    "#  - Transforming the outcome to a numpy vector\n",
    "\n",
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "# ### Input Data Normalization and Feature Engineering\n",
    "print('Training Data has been read and feature engineering is being performed....')\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "print(names)\n",
    "\n",
    "\n",
    "# ## Selecting Output for Component 1 of Stability Vector\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "count=1\n",
    "    \n",
    "y = df_train[stab_vec_list[count]]\n",
    "print(y.value_counts())\n",
    "\n",
    "stable_comp=df_train.loc[y==1,['formulaA','formulaB']] # Find the elements that create a stable element in this vector component\n",
    "print('Compound being analyzed is',stab_vec_list[count])\n",
    "stable_comp_num=stable_comp.values\n",
    "stable_A=np.unique(stable_comp_num[:,0])\n",
    "stable_B=np.unique(stable_comp_num[:,1])\n",
    "    \n",
    "df_unique= pd.DataFrame()\n",
    "\n",
    "y_unique= pd.DataFrame()\n",
    "    \n",
    "for cnt in range(stable_A.shape[0]):\n",
    "\n",
    "    df_tmp1=y.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "    y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "    df_tmp=df_train.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "    df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for cnt in range(stable_B.shape[0]):\n",
    "    df_tmp1=y.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "    y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "    df_tmp=df_train.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "    df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "\n",
    "    \n",
    "y_unique=y.iloc[y_unique.index.unique()]\n",
    "df_unique=df_train.iloc[df_unique.index.unique()]\n",
    "print(y_unique.value_counts())\n",
    "print('The elements in these compounds create a stable compound for this component of the stability vector:',y_unique.shape)\n",
    "    \n",
    "    \n",
    "y_stable=y_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "df_stable=df_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "print(y_stable.value_counts())\n",
    "print('The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound:',y_stable.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ## Pearson Correlation and Input Normalization\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# Pearson Correlation to Identify the features that influence the most on the output \n",
    "print('Pearson Correlation has been calculated to build the model in the most relevant features ....')\n",
    "X_train_new_all=df_stable[feature_cols] #This means we will only train on the elements that create a stable compound for this component of the stability vector and have at least one stable compound\n",
    "\n",
    "y_new=y_stable\n",
    "print('Number of Results to train on:',y_new.shape)\n",
    "print('Number of Training Features before Pearson correlation:', X_train_new_all.shape[1])\n",
    "\n",
    "corr_df=pd.concat([X_train_new_all, y_new],axis=1)\n",
    "a=corr_df.corr()\n",
    "#a['Stable_compunds'].hist(bins=7, figsize=(18, 12), xlabelsize=10)\n",
    "\n",
    "## Incorporating the Features that contribute the most based on a pearson correlation coefficient threshold\n",
    "\n",
    "thr=.09\n",
    "\n",
    "corr_variables=list(a[a[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "del(corr_variables[-1])\n",
    "\n",
    "\n",
    "print('Pearson Correlation has identified', len(corr_variables), 'with ', str(thr) )\n",
    "\n",
    "## Normalization of Input Data\n",
    "\n",
    "## Using Un-normalized data as input\n",
    "X_train_new=df_stable[corr_variables]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "\n",
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "## Using Pearson Correlation in PCA\n",
    "df1= pd.DataFrame(data=X_train_new_PCA, index=indx_4_PC)\n",
    "print(df1.shape)\n",
    "\n",
    "corr_df_PCA=pd.concat([df1, y_new],axis=1)\n",
    "\n",
    "\n",
    "a_PCA=corr_df_PCA.corr()\n",
    "\n",
    "thr=.05\n",
    "corr_variables_PCA=list(a_PCA[a_PCA[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "\n",
    "del(corr_variables_PCA[-1])\n",
    "\n",
    "print('Pearson Correlation in PCA Space has identified', len(corr_variables_PCA), 'with ', str(thr) )\n",
    "\n",
    "X_train_PCA_PC=df1[corr_variables_PCA]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation in PCA Space:', X_train_PCA_PC.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Model Generation\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "print('Training Model Using Z-normalized Data')\n",
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new_Z_score, y_new,\n",
    "                                                    test_size=.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Optimal Random Forest --\n",
      "Optimal precision:  0.3548387096774194   recall:  0.6875   F1:  0.4680851063829787   accuracy:  0.8161764705882353\n",
      "optimal Confusion matrix\n",
      "[[100  20]\n",
      " [  5  11]]\n",
      "Optimal AUC: 0.7604166666666666\n",
      " -- Optimal Decision Tree --\n",
      "Optimal precision:  0.34615384615384615   recall:  0.5625   F1:  0.4285714285714286   accuracy:  0.8235294117647058\n",
      "optimal Confusion matrix\n",
      "[[103  17]\n",
      " [  7   9]]\n",
      "Optimal AUC: 0.7104166666666667\n",
      " -- Optimal KNN --\n",
      "Optimal precision:  0.5294117647058824   recall:  0.5625   F1:  0.5454545454545455   accuracy:  0.8897058823529411\n",
      "optimal Confusion matrix\n",
      "[[112   8]\n",
      " [  7   9]]\n",
      "Optimal AUC: 0.7479166666666667\n",
      " -- Optimal SVM --\n",
      "Optimal precision:  0.36363636363636365   recall:  0.5   F1:  0.4210526315789474   accuracy:  0.8382352941176471\n",
      "optimal Confusion matrix\n",
      "[[106  14]\n",
      " [  8   8]]\n",
      "Optimal AUC: 0.6916666666666667\n",
      "-------- Bagging and Boosting Result-------\n",
      "------- ADA Boosting Random Forest Classifier-------\n",
      "Optimal precision:  0.6666666666666666   recall:  0.125   F1:  0.21052631578947367   accuracy:  0.8897058823529411\n",
      "optimal Confusion matrix\n",
      "[[119   1]\n",
      " [ 14   2]]\n",
      "Optimal AUC: 0.5583333333333333\n",
      "------- ADA Boosting Decision Tree Classifier-------\n",
      "Optimal precision:  0.3157894736842105   recall:  0.375   F1:  0.34285714285714286   accuracy:  0.8308823529411765\n",
      "optimal Confusion matrix\n",
      "[[107  13]\n",
      " [ 10   6]]\n",
      "Optimal AUC: 0.6333333333333333\n",
      "------- Gradient Boosting Classifier-------\n",
      "Optimal precision:  0.3333333333333333   recall:  0.5   F1:  0.4   accuracy:  0.8235294117647058\n",
      "optimal Confusion matrix\n",
      "[[104  16]\n",
      " [  8   8]]\n",
      "Optimal AUC: 0.6833333333333333\n",
      "------- Extra Trees Classifier-------\n",
      "Optimal precision:  0.13793103448275862   recall:  0.5   F1:  0.2162162162162162   accuracy:  0.5735294117647058\n",
      "optimal Confusion matrix\n",
      "[[70 50]\n",
      " [ 8  8]]\n",
      "Optimal AUC: 0.5416666666666666\n"
     ]
    }
   ],
   "source": [
    "## Fitting best Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "print(' -- Optimal Random Forest --')\n",
    "\n",
    "\n",
    "rfc_opt = RandomForestClassifier(n_estimators=1,\n",
    "                                 criterion='gini',\n",
    "                                 bootstrap=False,\n",
    "                                 max_depth=10, \n",
    "                                 class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                 min_samples_split=10,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 min_impurity_decrease=5e-7,\n",
    "                                 random_state=0,\n",
    "                                 n_jobs=-1)\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "y_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "## Fitting best Model\n",
    "print(' -- Optimal Decision Tree --')\n",
    "## Fitting best Model\n",
    "#[cr,max_d,sp,min_sample_sp,min_samples_le,min_impurity_sp]\n",
    "rfc_opt_DT = sklearn.tree.DecisionTreeClassifier(class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                                 criterion='gini',\n",
    "                                                 max_depth=100,\n",
    "                                                 random_state=0, \n",
    "                                                 splitter='random',\n",
    "                                                 min_samples_split=8,\n",
    "                                                 min_samples_leaf=1,\n",
    "                                                 min_impurity_decrease=5e-7)\n",
    "\n",
    "\n",
    "rfc_opt_DT.fit(X_train, y_train)\n",
    "y_pred = rfc_opt_DT.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print(' -- Optimal KNN --')\n",
    "rf_opt_KNN=KNeighborsClassifier(algorithm='auto',\n",
    "                                metric='minkowski',\n",
    "                                n_jobs=-1, \n",
    "                                n_neighbors=1,\n",
    "                                p=1,\n",
    "                                weights='distance')\n",
    "\n",
    "rf_opt_KNN.fit(X_train, y_train)\n",
    "y_pred = rf_opt_KNN.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print(' -- Optimal SVM --')\n",
    "\n",
    "\n",
    "rfc_opt_SVM = sklearn.svm.SVC(kernel='rbf', \n",
    "                      gamma=.1,C=15,\n",
    "                      random_state=0,\n",
    "                      class_weight={0:y_train.mean(), 1:1-y_train.mean()})\n",
    "\n",
    "rfc_opt_SVM.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_SVM.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print('-------- Bagging and Boosting Result-------')\n",
    "\n",
    "print('------- ADA Boosting Random Forest Classifier-------')\n",
    "\n",
    "#[estimator,cr,boots,max_d,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "\n",
    "rfc_opT_rf = RandomForestClassifier(n_estimators=1,\n",
    "                                 criterion='gini',\n",
    "                                 bootstrap=False,\n",
    "                                 max_depth=10, \n",
    "                                 class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                 min_samples_split=10,\n",
    "                                 min_samples_leaf=10,\n",
    "                                 min_impurity_decrease=5e-7,\n",
    "                                 random_state=0,\n",
    "                                 n_jobs=-1)\n",
    "clf_RF = AdaBoostClassifier(base_estimator=rfc_opT_rf,\n",
    "                         n_estimators=1000,\n",
    "                         learning_rate=.0001)\n",
    "\n",
    "clf_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_RF.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- ADA Boosting Decision Tree Classifier-------')\n",
    "\n",
    "#([cr,max_d,sp,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "\n",
    "rfc = sklearn.tree.DecisionTreeClassifier(class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                                 criterion='gini',\n",
    "                                                 max_depth=100,\n",
    "                                                 random_state=0, \n",
    "                                                 splitter='random',\n",
    "                                                 min_samples_split=8,\n",
    "                                                 min_samples_leaf=1,\n",
    "                                                 min_impurity_decrease=5e-7)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=rfc,\n",
    "                         n_estimators=1,\n",
    "                         learning_rate=10)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- Gradient Boosting Classifier-------')\n",
    "\n",
    "#[max_d,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "rfc_opt_GRAD=GradientBoostingClassifier(n_estimators=1,\n",
    "                                         learning_rate=10,\n",
    "                                         min_samples_split=8,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         max_depth=100,\n",
    "                                         random_state=0)\n",
    "\n",
    "rfc_opt_GRAD.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_GRAD.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- Extra Trees Classifier-------')\n",
    "\n",
    "#[estimator,cr,boots,max_d,min_sample_sp,min_samples_le,min_impurity_sp]\n",
    "\n",
    "rfc_opt_Extra= ExtraTreesClassifier(n_estimators=1,\n",
    "                                     criterion='gini',\n",
    "                                     bootstrap=False,\n",
    "                                     max_depth=10,\n",
    "                                     class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                     min_samples_split=10,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_impurity_decrease=5e-7,\n",
    "                                     random_state=0,n_jobs=-1)\n",
    "\n",
    "rfc_opt_Extra.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_Extra.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving the Best Model\n",
    "\n",
    "Selecting the model that has the highest AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Best_model_component_2.sav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "filename = 'Best_model_component_2.sav'\n",
    "model=rfc_opt\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
