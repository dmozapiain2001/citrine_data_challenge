{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data is being read ....\n",
      "(2572, 98)\n",
      "(2572, 109)\n",
      "Training Data has been read and feature engineering is being performed....\n",
      "(2572, 110)\n",
      "['training_data.csv', 'test_data.csv']\n",
      "0    1974\n",
      "1     598\n",
      "Name: A73B, dtype: int64\n",
      "Compound being analyzed is A73B\n",
      "0    1951\n",
      "1     598\n",
      "Name: A73B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector: (2549,)\n",
      "0    626\n",
      "1    598\n",
      "Name: A73B, dtype: int64\n",
      "The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound: (1224,)\n",
      "Pearson Correlation has been calculated to build the model in the most relevant features ....\n",
      "Number of Results to train on: (1224,)\n",
      "Number of Training Features before Pearson correlation: 98\n",
      "Pearson Correlation has identified 29 with  0.24\n",
      "Number of Training Features after Pearson correlation: 29\n",
      "(1224, 29)\n",
      "(1224, 29)\n",
      "(1224, 29)\n",
      "(1224, 29)\n",
      "(1224, 20)\n",
      "(1224, 20)\n",
      "Pearson Correlation in PCA Space has identified 11 with  0.05\n",
      "Number of Training Features after Pearson correlation in PCA Space: 11\n",
      "Training Model Using Z-normalized Data\n",
      "(1040, 29) (1040,)\n",
      "(184, 29) (184,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scores\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "\n",
    "# # Reading in the Data\n",
    "\n",
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "        \n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "df_train.shape\n",
    "\n",
    "\n",
    "# ## Data Manipulation\n",
    "print('Training Data is being read ....')\n",
    "#  - Transforming the outcome to a numpy vector\n",
    "\n",
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "# ### Input Data Normalization and Feature Engineering\n",
    "print('Training Data has been read and feature engineering is being performed....')\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "print(names)\n",
    "\n",
    "\n",
    "# ## Selecting Output for Component 1 of Stability Vector\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "count=2\n",
    "    \n",
    "y = df_train[stab_vec_list[count]]\n",
    "print(y.value_counts())\n",
    "\n",
    "stable_comp=df_train.loc[y==1,['formulaA','formulaB']] # Find the elements that create a stable element in this vector component\n",
    "print('Compound being analyzed is',stab_vec_list[count])\n",
    "stable_comp_num=stable_comp.values\n",
    "stable_A=np.unique(stable_comp_num[:,0])\n",
    "stable_B=np.unique(stable_comp_num[:,1])\n",
    "    \n",
    "df_unique= pd.DataFrame()\n",
    "\n",
    "y_unique= pd.DataFrame()\n",
    "    \n",
    "for cnt in range(stable_A.shape[0]):\n",
    "\n",
    "    df_tmp1=y.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "    y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "    df_tmp=df_train.loc[df_train['formulaA']==stable_A[cnt]]\n",
    "    df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for cnt in range(stable_B.shape[0]):\n",
    "    df_tmp1=y.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "    y_unique=pd.concat([y_unique, df_tmp1],axis=0)\n",
    "        \n",
    "    df_tmp=df_train.loc[df_train['formulaB']==stable_B[cnt]]\n",
    "    df_unique=pd.concat([df_unique, df_tmp],axis=0)\n",
    "\n",
    "    \n",
    "y_unique=y.iloc[y_unique.index.unique()]\n",
    "df_unique=df_train.iloc[df_unique.index.unique()]\n",
    "print(y_unique.value_counts())\n",
    "print('The elements in these compounds create a stable compound for this component of the stability vector:',y_unique.shape)\n",
    "    \n",
    "    \n",
    "y_stable=y_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "df_stable=df_unique.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "print(y_stable.value_counts())\n",
    "print('The elements in these compounds create a stable compound for this component of the stability vector and create at least one stable compound:',y_stable.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ## Pearson Correlation and Input Normalization\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# Pearson Correlation to Identify the features that influence the most on the output \n",
    "print('Pearson Correlation has been calculated to build the model in the most relevant features ....')\n",
    "X_train_new_all=df_stable[feature_cols] #This means we will only train on the elements that create a stable compound for this component of the stability vector and have at least one stable compound\n",
    "\n",
    "y_new=y_stable\n",
    "print('Number of Results to train on:',y_new.shape)\n",
    "print('Number of Training Features before Pearson correlation:', X_train_new_all.shape[1])\n",
    "\n",
    "corr_df=pd.concat([X_train_new_all, y_new],axis=1)\n",
    "a=corr_df.corr()\n",
    "#a['Stable_compunds'].hist(bins=7, figsize=(18, 12), xlabelsize=10)\n",
    "\n",
    "## Incorporating the Features that contribute the most based on a pearson correlation coefficient threshold\n",
    "\n",
    "thr=0.24\n",
    "\n",
    "corr_variables=list(a[a[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "del(corr_variables[-1])\n",
    "\n",
    "\n",
    "print('Pearson Correlation has identified', len(corr_variables), 'with ', str(thr) )\n",
    "\n",
    "## Normalization of Input Data\n",
    "\n",
    "## Using Un-normalized data as input\n",
    "X_train_new=df_stable[corr_variables]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "\n",
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "## Using Pearson Correlation in PCA\n",
    "df1= pd.DataFrame(data=X_train_new_PCA, index=indx_4_PC)\n",
    "print(df1.shape)\n",
    "\n",
    "corr_df_PCA=pd.concat([df1, y_new],axis=1)\n",
    "\n",
    "\n",
    "a_PCA=corr_df_PCA.corr()\n",
    "\n",
    "thr=.05\n",
    "corr_variables_PCA=list(a_PCA[a_PCA[stab_vec_list[count]].abs()>thr].index)\n",
    "\n",
    "\n",
    "del(corr_variables_PCA[-1])\n",
    "\n",
    "print('Pearson Correlation in PCA Space has identified', len(corr_variables_PCA), 'with ', str(thr) )\n",
    "\n",
    "X_train_PCA_PC=df1[corr_variables_PCA]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation in PCA Space:', X_train_PCA_PC.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Model Generation\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "print('Training Model Using Z-normalized Data')\n",
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new_Z_score, y_new,\n",
    "                                                    test_size=.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Optimal Random Forest --\n",
      "Optimal precision:  0.7403846153846154   recall:  0.8651685393258427   F1:  0.7979274611398964   accuracy:  0.7880434782608695\n",
      "optimal Confusion matrix\n",
      "[[68 27]\n",
      " [12 77]]\n",
      "Optimal AUC: 0.7904790065050267\n",
      " -- Optimal Decision Tree --\n",
      "Optimal precision:  0.72   recall:  0.8089887640449438   F1:  0.761904761904762   accuracy:  0.7554347826086957\n",
      "optimal Confusion matrix\n",
      "[[67 28]\n",
      " [17 72]]\n",
      "Optimal AUC: 0.7571259609698403\n",
      " -- Optimal KNN --\n",
      "Optimal precision:  0.7128712871287128   recall:  0.8089887640449438   F1:  0.7578947368421052   accuracy:  0.75\n",
      "optimal Confusion matrix\n",
      "[[66 29]\n",
      " [17 72]]\n",
      "Optimal AUC: 0.7518628030751034\n",
      " -- Optimal SVM --\n",
      "Optimal precision:  0.7352941176470589   recall:  0.8426966292134831   F1:  0.7853403141361257   accuracy:  0.7771739130434783\n",
      "optimal Confusion matrix\n",
      "[[68 27]\n",
      " [14 75]]\n",
      "Optimal AUC: 0.7792430514488469\n",
      "-------- Bagging and Boosting Result-------\n",
      "------- ADA Boosting Random Forest Classifier-------\n",
      "Optimal precision:  0.6923076923076923   recall:  0.8089887640449438   F1:  0.7461139896373058   accuracy:  0.7336956521739131\n",
      "optimal Confusion matrix\n",
      "[[63 32]\n",
      " [17 72]]\n",
      "Optimal AUC: 0.736073329390893\n",
      "------- ADA Boosting Decision Tree Classifier-------\n",
      "Optimal precision:  0.7171717171717171   recall:  0.797752808988764   F1:  0.7553191489361702   accuracy:  0.75\n",
      "optimal Confusion matrix\n",
      "[[67 28]\n",
      " [18 71]]\n",
      "Optimal AUC: 0.7515079834417504\n",
      "------- Gradient Boosting Classifier-------\n",
      "Optimal precision:  0.6923076923076923   recall:  0.7078651685393258   F1:  0.7   accuracy:  0.7065217391304348\n",
      "optimal Confusion matrix\n",
      "[[67 28]\n",
      " [26 63]]\n",
      "Optimal AUC: 0.7065641632170313\n",
      "------- Extra Trees Classifier-------\n",
      "Optimal precision:  0.66   recall:  0.7415730337078652   F1:  0.6984126984126984   accuracy:  0.6902173913043478\n",
      "optimal Confusion matrix\n",
      "[[61 34]\n",
      " [23 66]]\n",
      "Optimal AUC: 0.69183914843288\n"
     ]
    }
   ],
   "source": [
    "## Fitting best Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "print(' -- Optimal Random Forest --')\n",
    "\n",
    "\n",
    "rfc_opt = RandomForestClassifier(n_estimators=3,\n",
    "                                 criterion='gini',\n",
    "                                 bootstrap=False,\n",
    "                                 max_depth=10, \n",
    "                                 class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                 min_samples_split=6,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 min_impurity_decrease=5e-7,\n",
    "                                 random_state=0,\n",
    "                                 n_jobs=-1)\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "y_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "## Fitting best Model\n",
    "print(' -- Optimal Decision Tree --')\n",
    "## Fitting best Model\n",
    "#[cr,max_d,sp,min_sample_sp,min_samples_le,min_impurity_sp]\n",
    "rfc_opt_DT = sklearn.tree.DecisionTreeClassifier(class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                                 criterion='gini',\n",
    "                                                 max_depth=100,\n",
    "                                                 random_state=0, \n",
    "                                                 splitter='random',\n",
    "                                                 min_samples_split=3,\n",
    "                                                 min_samples_leaf=1,\n",
    "                                                 min_impurity_decrease=5e-7)\n",
    "\n",
    "\n",
    "rfc_opt_DT.fit(X_train, y_train)\n",
    "y_pred = rfc_opt_DT.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print(' -- Optimal KNN --')\n",
    "rf_opt_KNN=KNeighborsClassifier(algorithm='auto',\n",
    "                                metric='minkowski',\n",
    "                                n_jobs=-1, \n",
    "                                n_neighbors=10,\n",
    "                                p=1,\n",
    "                                weights='distance')\n",
    "\n",
    "rf_opt_KNN.fit(X_train, y_train)\n",
    "y_pred = rf_opt_KNN.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print(' -- Optimal SVM --')\n",
    "\n",
    "\n",
    "rfc_opt_SVM = sklearn.svm.SVC(kernel='poly', \n",
    "                      gamma=3,C=.0001,\n",
    "                      random_state=0,\n",
    "                      class_weight={0:y_train.mean(), 1:1-y_train.mean()})\n",
    "\n",
    "rfc_opt_SVM.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_SVM.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print('-------- Bagging and Boosting Result-------')\n",
    "\n",
    "print('------- ADA Boosting Random Forest Classifier-------')\n",
    "\n",
    "#[estimator,cr,boots,max_d,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "\n",
    "rfc_opT_rf =  RandomForestClassifier(n_estimators=3,\n",
    "                                 criterion='gini',\n",
    "                                 bootstrap=False,\n",
    "                                 max_depth=10, \n",
    "                                 class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                 min_samples_split=6,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 min_impurity_decrease=5e-7,\n",
    "                                 random_state=0,\n",
    "                                 n_jobs=-1)\n",
    "clf_RF = AdaBoostClassifier(base_estimator=rfc_opT_rf,\n",
    "                         n_estimators=10,\n",
    "                         learning_rate=.0001)\n",
    "\n",
    "clf_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_RF.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- ADA Boosting Decision Tree Classifier-------')\n",
    "\n",
    "#([cr,max_d,sp,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "\n",
    "rfc = sklearn.tree.DecisionTreeClassifier(class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                                 criterion='gini',\n",
    "                                                 max_depth=100,\n",
    "                                                 random_state=0, \n",
    "                                                 splitter='random',\n",
    "                                                 min_samples_split=3,\n",
    "                                                 min_samples_leaf=1,\n",
    "                                                 min_impurity_decrease=5e-7)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=rfc,\n",
    "                         n_estimators=700,\n",
    "                         learning_rate=.1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- Gradient Boosting Classifier-------')\n",
    "\n",
    "#[max_d,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "rfc_opt_GRAD=GradientBoostingClassifier(n_estimators=1000,\n",
    "                                         learning_rate=.01,\n",
    "                                         min_samples_split=3,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         max_depth=100,\n",
    "                                         random_state=0)\n",
    "\n",
    "rfc_opt_GRAD.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_GRAD.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- Extra Trees Classifier-------')\n",
    "\n",
    "#[estimator,cr,boots,max_d,min_sample_sp,min_samples_le,min_impurity_sp]\n",
    "\n",
    "rfc_opt_Extra= ExtraTreesClassifier(n_estimators=3,\n",
    "                                     criterion='gini',\n",
    "                                     bootstrap=False,\n",
    "                                     max_depth=10,\n",
    "                                     class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                     min_samples_split=6,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_impurity_decrease=5e-7,\n",
    "                                     random_state=0,n_jobs=-1)\n",
    "\n",
    "rfc_opt_Extra.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_Extra.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Best Model\n",
    "\n",
    "Selecting the model that has the highest AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Best_model_component_3.sav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "filename = 'Best_model_component_3.sav'\n",
    "model=rfc_opt\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
