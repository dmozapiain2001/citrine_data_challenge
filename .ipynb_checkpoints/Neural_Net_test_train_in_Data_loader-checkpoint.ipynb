{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import scipy.ndimage as ndimage\n",
    "from torchvision import transforms, utils\n",
    "from toolz.curried import pipe, curry, compose\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "import scores\n",
    "\n",
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['training_data.csv', 'test_data.csv']\n",
      "(2572, 99)\n",
      "/Users/davidmontesdeoca/Desktop/challenge_data/data/training_data.csv\n"
     ]
    }
   ],
   "source": [
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "\n",
    "print(names)\n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "print(df_train.shape)\n",
    "print(path_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2572, 110)\n",
      "(2572, 110)\n",
      "(1228, 110)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>formulaA</th>\n",
       "      <th>formulaB</th>\n",
       "      <th>formulaA_elements_AtomicVolume</th>\n",
       "      <th>formulaB_elements_AtomicVolume</th>\n",
       "      <th>formulaA_elements_AtomicWeight</th>\n",
       "      <th>formulaB_elements_AtomicWeight</th>\n",
       "      <th>formulaA_elements_BoilingT</th>\n",
       "      <th>formulaB_elements_BoilingT</th>\n",
       "      <th>formulaA_elements_BulkModulus</th>\n",
       "      <th>formulaB_elements_BulkModulus</th>\n",
       "      <th>...</th>\n",
       "      <th>A82B</th>\n",
       "      <th>A73B</th>\n",
       "      <th>A64B</th>\n",
       "      <th>A55B</th>\n",
       "      <th>A46B</th>\n",
       "      <th>A37B</th>\n",
       "      <th>A28B</th>\n",
       "      <th>A19B</th>\n",
       "      <th>B</th>\n",
       "      <th>Stable_compunds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>47</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>17.075648</td>\n",
       "      <td>227.0</td>\n",
       "      <td>107.868200</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>13</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>16.594425</td>\n",
       "      <td>227.0</td>\n",
       "      <td>26.981539</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2792.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>89</td>\n",
       "      <td>33</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>21.723966</td>\n",
       "      <td>227.0</td>\n",
       "      <td>74.921600</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89</td>\n",
       "      <td>56</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>64.969282</td>\n",
       "      <td>227.0</td>\n",
       "      <td>137.327000</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>2143.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>89</td>\n",
       "      <td>83</td>\n",
       "      <td>37.433086</td>\n",
       "      <td>35.483459</td>\n",
       "      <td>227.0</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>3473.0</td>\n",
       "      <td>1837.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   formulaA  formulaB  formulaA_elements_AtomicVolume  \\\n",
       "0        89        47                       37.433086   \n",
       "1        89        13                       37.433086   \n",
       "2        89        33                       37.433086   \n",
       "3        89        56                       37.433086   \n",
       "4        89        83                       37.433086   \n",
       "\n",
       "   formulaB_elements_AtomicVolume  formulaA_elements_AtomicWeight  \\\n",
       "0                       17.075648                           227.0   \n",
       "1                       16.594425                           227.0   \n",
       "2                       21.723966                           227.0   \n",
       "3                       64.969282                           227.0   \n",
       "4                       35.483459                           227.0   \n",
       "\n",
       "   formulaB_elements_AtomicWeight  formulaA_elements_BoilingT  \\\n",
       "0                      107.868200                      3473.0   \n",
       "1                       26.981539                      3473.0   \n",
       "2                       74.921600                      3473.0   \n",
       "3                      137.327000                      3473.0   \n",
       "4                      208.980400                      3473.0   \n",
       "\n",
       "   formulaB_elements_BoilingT  formulaA_elements_BulkModulus  \\\n",
       "0                      2435.0                            0.0   \n",
       "1                      2792.0                            0.0   \n",
       "2                       887.0                            0.0   \n",
       "3                      2143.0                            0.0   \n",
       "4                      1837.0                            0.0   \n",
       "\n",
       "   formulaB_elements_BulkModulus       ...         A82B  A73B  A64B  A55B  \\\n",
       "0                          100.0       ...            0     1     0     1   \n",
       "1                           76.0       ...            0     1     0     0   \n",
       "2                           22.0       ...            0     0     0     0   \n",
       "3                            9.6       ...            0     0     0     0   \n",
       "4                           31.0       ...            0     0     0     0   \n",
       "\n",
       "   A46B  A37B  A28B  A19B  B  Stable_compunds  \n",
       "0     0     0     0     0  1                1  \n",
       "1     0     0     0     0  1                1  \n",
       "2     0     0     0     0  1                0  \n",
       "3     0     0     0     0  1                0  \n",
       "4     0     0     0     0  1                0  \n",
       "\n",
       "[5 rows x 110 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','Stable_compunds']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "\n",
    "y_all=df_train[stab_vec_list[0:-1]]\n",
    "\n",
    "\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "print(df_train.shape)\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "\n",
    "df_stable=df_train.loc[np.logical_not(y_all.sum(axis=1)==0)]\n",
    "print(df_stable.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_new=df_stable[feature_cols] #training only on stable elements\n",
    "#y_target=df_stable[stab_vec_list]\n",
    "###\n",
    "\n",
    "X_train_new=df_train[feature_cols]   #training  on all elements\n",
    "y_target=df_train[stab_vec_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2522\n",
       "1      50\n",
       "Name: A91B, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target[stab_vec_list[0]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2572, 98)\n",
      "(2572, 98)\n",
      "(2572, 98)\n",
      "(2572, 98)\n",
      "(1228, 20)\n",
      "(2572, 98)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "X_train_new_mag_1=pd.DataFrame(data=X_train_new_mag_1,columns=feature_cols)\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "X_train_new_0_1=pd.DataFrame(data=X_train_new_0_1,columns=feature_cols)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "X_train_new_m1_p1=pd.DataFrame(data=X_train_new_m1_p1,columns=feature_cols)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_stable[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "\n",
    "## Taking Logarithm of High Values\n",
    "\n",
    "X_train_new_log=X_train_new.copy()\n",
    "X_train_new_log[X_train_new_log>100]=X_train_new_log[X_train_new_log>100].apply(np.log)\n",
    "print(X_train_new_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A91B</th>\n",
       "      <th>A82B</th>\n",
       "      <th>A73B</th>\n",
       "      <th>A64B</th>\n",
       "      <th>A55B</th>\n",
       "      <th>A46B</th>\n",
       "      <th>A37B</th>\n",
       "      <th>A28B</th>\n",
       "      <th>A19B</th>\n",
       "      <th>Stable_compunds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A91B  A82B  A73B  A64B  A55B  A46B  A37B  A28B  A19B  Stable_compunds\n",
       "0     0     0     1     0     1     0     0     0     0                1\n",
       "1     0     0     1     0     0     0     0     0     0                1\n",
       "2     0     0     0     0     0     0     0     0     0                0\n",
       "3     0     0     0     0     0     0     0     0     0                0\n",
       "4     0     0     0     0     0     0     0     0     0                0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument_group('Optimization related arguments')\n",
    "parser.add_argument('-num_epochs', default=50, type=int, help='Epochs')\n",
    "parser.add_argument('-batch_size', default=50, type=int, help='Batch size')\n",
    "parser.add_argument('-lr', default=1e-4, type=float, help='Learning rate')\n",
    "parser.add_argument('-lr_decay_rate', default=0.9997592083, type=float, help='Decay for lr')\n",
    "parser.add_argument('-min_lr', default=5e-5, type=float, help='Minimum learning rate')\n",
    "parser.add_argument('-weight_init', default='kaiming', choices=['xavier', 'kaiming'], help='Weight initialization strategy')\n",
    "parser.add_argument('-overfit', action='store_true', help='Overfit on 5 examples, meant for debugging')\n",
    "parser.add_argument('-gpuid', default=-1, type=int, help='GPU id to use')\n",
    "        \n",
    "parser.add_argument('-test_size', default=0.15)\n",
    "\n",
    "parser.add_argument_group('Checkpointing related arguments')\n",
    "parser.add_argument('-load_path', default='', help='Checkpoint to load path from')\n",
    "parser.add_argument('-save_path', default='checkpoints/', help='Path to save checkpoints')\n",
    "parser.add_argument('-save_step', default=2, type=int, help='Save checkpoint after every save_step epochs')\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# input arguments and options\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_decay_rate       : 0.9997592083\n",
      "num_epochs          : 50\n",
      "load_path           : \n",
      "min_lr              : 5e-05\n",
      "overfit             : False\n",
      "save_step           : 2\n",
      "weight_init         : kaiming\n",
      "save_path           : checkpoints/23-Jan-2019-23:37:53\n",
      "test_size           : 0.15\n",
      "batch_size          : 50\n",
      "gpuid               : -1\n",
      "lr                  : 0.0001\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.strftime(datetime.datetime.utcnow(), '%d-%b-%Y-%H:%M:%S')\n",
    "if args.save_path == 'checkpoints/':\n",
    "    args.save_path += start_time\n",
    "    \n",
    "if args.load_path != '':\n",
    "    components = torch.load(args.load_path)\n",
    "    model_args = components['model_args']\n",
    "    model_args.gpuid = args.gpuid\n",
    "    model_args.batch_size = args.batch_size\n",
    "\n",
    "    # this is required by dataloader\n",
    "    args.normalize = model_args.normalize\n",
    "\n",
    "for arg in vars(args):\n",
    "    print('{:<20}: {}'.format(arg, getattr(args, arg)))\n",
    "    \n",
    "# transfer all options to model\n",
    "model_args = args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for reproducibility\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# set device and default tensor type\n",
    "if args.gpuid >= 0:\n",
    "    torch.cuda.manual_seed_all(1234)\n",
    "    torch.cuda.set_device(args.gpuid)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class arg_dataset(Dataset):\n",
    "    \"\"\"Face landmarks Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, data, y_true,args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"        \n",
    "        self.data=data\n",
    "        self.y_true=y_true\n",
    "        self.args = args\n",
    "        assert len(data) == len(y_true)\n",
    "        \n",
    "    \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, y_true,\n",
    "                                                            test_size=args.test_size,\n",
    "                                                            shuffle=True,\n",
    "                                                            random_state=42)\n",
    "        self.data = data\n",
    "        self.y_true = y_true\n",
    "\n",
    "        self.X_train = torch.from_numpy(X_train.values).float()\n",
    "        self.y_train = torch.from_numpy(y_train.values).float()\n",
    "        self.X_test = torch.from_numpy(X_test.values).float()\n",
    "        self.y_test = torch.from_numpy(y_test.values).float()\n",
    "\n",
    "        self.num_data_points = {}\n",
    "        self.num_data_points['train'] = len(X_train)\n",
    "        self.num_data_points['test'] = len(X_test)\n",
    "        \n",
    "        self._split = 'train'\n",
    "\n",
    "    @property\n",
    "    def split(self):\n",
    "        return self._split\n",
    "\n",
    "    @split.setter\n",
    "    def split(self, split):\n",
    "        self._split = split\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # methods to override - __len__ and __getitem__ methods\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data_points[self._split]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dtype = self._split\n",
    "        item = {'index': idx}\n",
    "        item['features'] = self.X_train[idx,:]\n",
    "        item['outputs'] = self.y_train[idx,:]\n",
    "        return item\n",
    "\n",
    "    #-------------------------------------------------------------------------\n",
    "    # collate function utilized by dataloader for batching\n",
    "    #-------------------------------------------------------------------------\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        dtype = self._split\n",
    "        merged_batch = {key: [d[key] for d in batch] for key in batch[0]}\n",
    "        out = {}\n",
    "        for key in merged_batch:\n",
    "            if key in {'index'}:\n",
    "                out[key] = merged_batch[key]\n",
    "            else:\n",
    "                out[key] = torch.stack(merged_batch[key], 0)\n",
    "\n",
    "        batch_keys = ['features', 'outputs']\n",
    "        return {key: out[key] for key in batch_keys}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(98, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(120, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(60, 30),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(30, 10)\n",
    "        )\n",
    "    def forward(self, batch):\n",
    "        return self.layers(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = arg_dataset(X_train_new_m1_p1, y_target,args)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 iter per epoch.\n"
     ]
    }
   ],
   "source": [
    "setattr(args, 'iter_per_epoch', math.ceil(dataset.num_data_points['train'] / args.batch_size))\n",
    "print(\"{} iter per epoch.\".format(args.iter_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(model_args)\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=args.lr_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.gpuid >= 0:\n",
    "    net = net.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start time: 24-Jan-2019-01:34:13\n",
      "[0:00:00.331805][Epoch:   1][Iter:      6][Loss: 0.691444][val loss: 0.686079][accuracy: 0.000000][lr: 0.001000]\n",
      "[0:00:03.389948][Epoch:   1][Iter:      6][Loss: 0.488948][val loss: 0.347820][accuracy: 0.568571][lr: 0.000990]\n",
      "[0:00:03.788743][Epoch:   2][Iter:     50][Loss: 0.464578][val loss: 0.341438][accuracy: 0.568571][lr: 0.000989]\n",
      "[0:00:06.608134][Epoch:   2][Iter:     50][Loss: 0.362055][val loss: 0.331309][accuracy: 0.568571][lr: 0.000980]\n",
      "[0:00:06.867792][Epoch:   3][Iter:     94][Loss: 0.360381][val loss: 0.334127][accuracy: 0.000000][lr: 0.000979]\n",
      "[0:00:09.686650][Epoch:   3][Iter:     94][Loss: 0.346633][val loss: 0.328019][accuracy: 0.277143][lr: 0.000970]\n",
      "[0:00:09.984580][Epoch:   4][Iter:    138][Loss: 0.345996][val loss: 0.326696][accuracy: 0.554286][lr: 0.000969]\n",
      "[0:00:12.729080][Epoch:   4][Iter:    138][Loss: 0.328386][val loss: 0.317749][accuracy: 0.537143][lr: 0.000959]\n",
      "[0:00:13.025393][Epoch:   5][Iter:    182][Loss: 0.329074][val loss: 0.316854][accuracy: 0.517143][lr: 0.000959]\n",
      "[0:00:15.824753][Epoch:   5][Iter:    182][Loss: 0.315425][val loss: 0.305579][accuracy: 0.420000][lr: 0.000949]\n",
      "[0:00:16.094298][Epoch:   6][Iter:    226][Loss: 0.315763][val loss: 0.303936][accuracy: 0.420000][lr: 0.000948]\n",
      "[0:00:18.947899][Epoch:   6][Iter:    226][Loss: 0.302079][val loss: 0.288413][accuracy: 0.405714][lr: 0.000939]\n",
      "[0:00:19.254026][Epoch:   7][Iter:    270][Loss: 0.298977][val loss: 0.288035][accuracy: 0.385714][lr: 0.000938]\n",
      "[0:00:21.933543][Epoch:   7][Iter:    270][Loss: 0.277114][val loss: 0.271044][accuracy: 0.408571][lr: 0.000929]\n",
      "[0:00:22.255274][Epoch:   8][Iter:    314][Loss: 0.278954][val loss: 0.270333][accuracy: 0.402857][lr: 0.000929]\n",
      "[0:00:25.337861][Epoch:   8][Iter:    314][Loss: 0.264876][val loss: 0.256870][accuracy: 0.420000][lr: 0.000920]\n",
      "[0:00:25.657438][Epoch:   9][Iter:    358][Loss: 0.260869][val loss: 0.257845][accuracy: 0.402857][lr: 0.000919]\n",
      "[0:00:28.681099][Epoch:   9][Iter:    358][Loss: 0.245706][val loss: 0.253051][accuracy: 0.485714][lr: 0.000910]\n",
      "[0:00:28.969610][Epoch:  10][Iter:    402][Loss: 0.245069][val loss: 0.251931][accuracy: 0.440000][lr: 0.000909]\n",
      "[0:00:31.903971][Epoch:  10][Iter:    402][Loss: 0.244326][val loss: 0.250425][accuracy: 0.425714][lr: 0.000900]\n",
      "[0:00:32.219301][Epoch:  11][Iter:    446][Loss: 0.243341][val loss: 0.245353][accuracy: 0.448571][lr: 0.000899]\n",
      "[0:00:35.027050][Epoch:  11][Iter:    446][Loss: 0.236420][val loss: 0.242312][accuracy: 0.462857][lr: 0.000891]\n",
      "[0:00:35.291791][Epoch:  12][Iter:    490][Loss: 0.238347][val loss: 0.241581][accuracy: 0.491429][lr: 0.000890]\n",
      "[0:00:38.069025][Epoch:  12][Iter:    490][Loss: 0.233302][val loss: 0.239186][accuracy: 0.468571][lr: 0.000881]\n",
      "[0:00:38.367089][Epoch:  13][Iter:    534][Loss: 0.231438][val loss: 0.238682][accuracy: 0.482857][lr: 0.000881]\n",
      "[0:00:41.245782][Epoch:  13][Iter:    534][Loss: 0.227894][val loss: 0.236446][accuracy: 0.474286][lr: 0.000872]\n",
      "[0:00:41.563258][Epoch:  14][Iter:    578][Loss: 0.223623][val loss: 0.234147][accuracy: 0.514286][lr: 0.000871]\n",
      "[0:00:44.464534][Epoch:  14][Iter:    578][Loss: 0.225789][val loss: 0.238594][accuracy: 0.457143][lr: 0.000863]\n",
      "[0:00:44.784676][Epoch:  15][Iter:    622][Loss: 0.226441][val loss: 0.234257][accuracy: 0.497143][lr: 0.000862]\n",
      "[0:00:47.641890][Epoch:  15][Iter:    622][Loss: 0.219056][val loss: 0.229444][accuracy: 0.525714][lr: 0.000854]\n",
      "[0:00:47.948561][Epoch:  16][Iter:    666][Loss: 0.226713][val loss: 0.229678][accuracy: 0.505714][lr: 0.000853]\n",
      "[0:00:50.918900][Epoch:  16][Iter:    666][Loss: 0.216528][val loss: 0.228852][accuracy: 0.491429][lr: 0.000845]\n",
      "[0:00:51.247756][Epoch:  17][Iter:    710][Loss: 0.212633][val loss: 0.228489][accuracy: 0.542857][lr: 0.000844]\n",
      "[0:00:54.094997][Epoch:  17][Iter:    710][Loss: 0.216701][val loss: 0.226946][accuracy: 0.528571][lr: 0.000836]\n",
      "[0:00:54.360606][Epoch:  18][Iter:    754][Loss: 0.220383][val loss: 0.227013][accuracy: 0.540000][lr: 0.000835]\n",
      "[0:00:57.164839][Epoch:  18][Iter:    754][Loss: 0.209270][val loss: 0.224729][accuracy: 0.542857][lr: 0.000827]\n",
      "[0:00:57.483070][Epoch:  19][Iter:    798][Loss: 0.207625][val loss: 0.226625][accuracy: 0.500000][lr: 0.000826]\n",
      "[0:01:01.270737][Epoch:  19][Iter:    798][Loss: 0.208432][val loss: 0.220744][accuracy: 0.557143][lr: 0.000818]\n",
      "[0:01:01.624908][Epoch:  20][Iter:    842][Loss: 0.207962][val loss: 0.223847][accuracy: 0.505714][lr: 0.000818]\n",
      "[0:01:05.348279][Epoch:  20][Iter:    842][Loss: 0.211360][val loss: 0.222053][accuracy: 0.565714][lr: 0.000810]\n",
      "[0:01:05.823305][Epoch:  21][Iter:    886][Loss: 0.207449][val loss: 0.221630][accuracy: 0.554286][lr: 0.000809]\n",
      "[0:01:09.352137][Epoch:  21][Iter:    886][Loss: 0.209244][val loss: 0.221400][accuracy: 0.514286][lr: 0.000801]\n",
      "[0:01:09.655898][Epoch:  22][Iter:    930][Loss: 0.206007][val loss: 0.222791][accuracy: 0.557143][lr: 0.000801]\n",
      "[0:01:12.455543][Epoch:  22][Iter:    930][Loss: 0.207950][val loss: 0.216113][accuracy: 0.554286][lr: 0.000793]\n",
      "[0:01:12.748729][Epoch:  23][Iter:    974][Loss: 0.208460][val loss: 0.216538][accuracy: 0.551429][lr: 0.000792]\n",
      "[0:01:15.557540][Epoch:  23][Iter:    974][Loss: 0.204966][val loss: 0.215221][accuracy: 0.545714][lr: 0.000784]\n",
      "[0:01:15.867164][Epoch:  24][Iter:   1018][Loss: 0.202994][val loss: 0.215513][accuracy: 0.542857][lr: 0.000784]\n",
      "[0:01:19.733762][Epoch:  24][Iter:   1018][Loss: 0.202085][val loss: 0.216652][accuracy: 0.545714][lr: 0.000776]\n",
      "[0:01:20.303049][Epoch:  25][Iter:   1062][Loss: 0.204410][val loss: 0.218171][accuracy: 0.525714][lr: 0.000775]\n",
      "[0:01:25.244038][Epoch:  25][Iter:   1062][Loss: 0.199361][val loss: 0.213791][accuracy: 0.537143][lr: 0.000768]\n",
      "[0:01:25.710295][Epoch:  26][Iter:   1106][Loss: 0.200802][val loss: 0.214252][accuracy: 0.557143][lr: 0.000767]\n",
      "[0:01:29.426030][Epoch:  26][Iter:   1106][Loss: 0.195328][val loss: 0.213589][accuracy: 0.551429][lr: 0.000760]\n",
      "[0:01:30.015181][Epoch:  27][Iter:   1150][Loss: 0.199646][val loss: 0.217641][accuracy: 0.537143][lr: 0.000759]\n",
      "[0:01:34.000841][Epoch:  27][Iter:   1150][Loss: 0.199630][val loss: 0.214945][accuracy: 0.545714][lr: 0.000752]\n",
      "[0:01:34.348148][Epoch:  28][Iter:   1194][Loss: 0.200851][val loss: 0.213282][accuracy: 0.542857][lr: 0.000751]\n",
      "[0:01:37.242254][Epoch:  28][Iter:   1194][Loss: 0.196134][val loss: 0.210684][accuracy: 0.551429][lr: 0.000744]\n",
      "[0:01:37.554900][Epoch:  29][Iter:   1238][Loss: 0.194382][val loss: 0.211637][accuracy: 0.540000][lr: 0.000743]\n",
      "[0:01:40.449936][Epoch:  29][Iter:   1238][Loss: 0.195021][val loss: 0.212627][accuracy: 0.537143][lr: 0.000736]\n",
      "[0:01:40.757908][Epoch:  30][Iter:   1282][Loss: 0.192363][val loss: 0.210077][accuracy: 0.551429][lr: 0.000735]\n",
      "[0:01:43.691384][Epoch:  30][Iter:   1282][Loss: 0.190799][val loss: 0.210047][accuracy: 0.548571][lr: 0.000728]\n",
      "[0:01:43.999609][Epoch:  31][Iter:   1326][Loss: 0.190424][val loss: 0.211214][accuracy: 0.554286][lr: 0.000728]\n",
      "[0:01:46.940948][Epoch:  31][Iter:   1326][Loss: 0.190025][val loss: 0.208721][accuracy: 0.548571][lr: 0.000721]\n",
      "[0:01:47.249710][Epoch:  32][Iter:   1370][Loss: 0.189376][val loss: 0.212348][accuracy: 0.565714][lr: 0.000720]\n",
      "[0:01:50.253339][Epoch:  32][Iter:   1370][Loss: 0.186438][val loss: 0.207833][accuracy: 0.565714][lr: 0.000713]\n",
      "[0:01:50.618248][Epoch:  33][Iter:   1414][Loss: 0.191812][val loss: 0.207261][accuracy: 0.545714][lr: 0.000712]\n",
      "[0:01:53.467544][Epoch:  33][Iter:   1414][Loss: 0.182669][val loss: 0.208423][accuracy: 0.548571][lr: 0.000706]\n",
      "[0:01:53.766174][Epoch:  34][Iter:   1458][Loss: 0.183794][val loss: 0.212128][accuracy: 0.537143][lr: 0.000705]\n",
      "[0:01:56.596586][Epoch:  34][Iter:   1458][Loss: 0.186788][val loss: 0.207021][accuracy: 0.554286][lr: 0.000698]\n",
      "[0:01:56.951456][Epoch:  35][Iter:   1502][Loss: 0.184864][val loss: 0.208411][accuracy: 0.551429][lr: 0.000697]\n",
      "[0:01:59.924152][Epoch:  35][Iter:   1502][Loss: 0.192574][val loss: 0.206001][accuracy: 0.565714][lr: 0.000691]\n",
      "[0:02:00.230242][Epoch:  36][Iter:   1546][Loss: 0.188274][val loss: 0.204504][accuracy: 0.562857][lr: 0.000690]\n",
      "[0:02:03.845449][Epoch:  36][Iter:   1546][Loss: 0.186927][val loss: 0.205046][accuracy: 0.540000][lr: 0.000684]\n",
      "[0:02:04.198940][Epoch:  37][Iter:   1590][Loss: 0.187547][val loss: 0.203631][accuracy: 0.557143][lr: 0.000683]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0:02:07.081278][Epoch:  37][Iter:   1590][Loss: 0.186456][val loss: 0.206193][accuracy: 0.568571][lr: 0.000676]\n",
      "[0:02:07.374444][Epoch:  38][Iter:   1634][Loss: 0.187667][val loss: 0.203699][accuracy: 0.540000][lr: 0.000676]\n",
      "[0:02:10.555530][Epoch:  38][Iter:   1634][Loss: 0.181243][val loss: 0.208436][accuracy: 0.574286][lr: 0.000669]\n",
      "[0:02:10.848650][Epoch:  39][Iter:   1678][Loss: 0.181679][val loss: 0.205062][accuracy: 0.540000][lr: 0.000669]\n",
      "[0:02:13.598473][Epoch:  39][Iter:   1678][Loss: 0.179538][val loss: 0.205146][accuracy: 0.557143][lr: 0.000662]\n",
      "[0:02:13.878538][Epoch:  40][Iter:   1722][Loss: 0.180668][val loss: 0.206845][accuracy: 0.551429][lr: 0.000661]\n",
      "[0:02:16.976909][Epoch:  40][Iter:   1722][Loss: 0.181464][val loss: 0.204049][accuracy: 0.545714][lr: 0.000655]\n",
      "[0:02:17.300159][Epoch:  41][Iter:   1766][Loss: 0.188670][val loss: 0.218147][accuracy: 0.494286][lr: 0.000655]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-9f59f9b88bf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# --------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# training\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "net.train()\n",
    "os.makedirs(args.save_path, exist_ok=True)\n",
    "\n",
    "running_loss = 0.0\n",
    "train_begin = datetime.datetime.utcnow()\n",
    "print(\"Training start time: {}\".format(datetime.datetime.strftime(train_begin, '%d-%b-%Y-%H:%M:%S')))\n",
    "\n",
    "log_loss = []\n",
    "for epoch in range(1, model_args.num_epochs + 1):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        for key in batch:\n",
    "            batch[key] = Variable(batch[key])\n",
    "            if args.gpuid >= 0:\n",
    "                batch[key] = batch[key].cuda()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # forward-backward pass and optimizer step\n",
    "        # --------------------------------------------------------------------\n",
    "        net_out = net(batch['features'])\n",
    "\n",
    "        cur_loss = criterion(net_out, batch['outputs'])\n",
    "        cur_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        gc.collect()\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # update running loss and decay learning rates\n",
    "        # --------------------------------------------------------------------\n",
    "        #train_loss = cur_loss.data[0]\n",
    "        train_loss = cur_loss.item()\n",
    "        if running_loss > 0.0:\n",
    "            running_loss = 0.95 * running_loss + 0.05 * train_loss\n",
    "        else:\n",
    "            running_loss = train_loss \n",
    "\n",
    "        if optimizer.param_groups[0]['lr'] > args.min_lr:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # print after every few iterations\n",
    "        # --------------------------------------------------------------------\n",
    "        if i % 40 == 0:\n",
    "            test_losses = []\n",
    "            accuracy = []\n",
    "            for i in range(int(dataset.num_data_points['test']/args.batch_size)):\n",
    "                test_feat = dataset.X_test[i*args.batch_size:(i+1)*args.batch_size, :]\n",
    "                test_labels = dataset.y_test[i*args.batch_size:(i+1)*args.batch_size, :]\n",
    "                test_feat = Variable(test_feat)\n",
    "                test_labels = Variable(test_labels)\n",
    "                if args.gpuid >= 0:\n",
    "                    test_feat = test_feat.cuda()\n",
    "                    test_labels = test_labels.cuda()\n",
    "                net.eval()\n",
    "                net_out = net(test_feat)\n",
    "                cur_loss = criterion(net_out, test_labels)\n",
    "                test_losses.append(cur_loss.item())\n",
    "                \n",
    "                y_pred = torch.sigmoid(net_out).data > 0.5\n",
    "                y_pred = y_pred.cpu().numpy()\n",
    "                accuracy.append((test_labels.cpu().numpy() == y_pred).all(axis=1))\n",
    "\n",
    "            validation_loss = np.mean(test_losses)\n",
    "            \n",
    "            accuracy = np.mean(accuracy)\n",
    "\n",
    "            iteration = (epoch - 1) * args.iter_per_epoch + i\n",
    "\n",
    "            log_loss.append((epoch,\n",
    "                             iteration,\n",
    "                             running_loss,\n",
    "                             train_loss,\n",
    "                             validation_loss,\n",
    "                             accuracy,\n",
    "                             optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            # print current time, running average, learning rate, iteration, epoch\n",
    "            print(\"[{}][Epoch: {:3d}][Iter: {:6d}][Loss: {:6f}][val loss: {:6f}][accuracy: {:6f}][lr: {:7f}]\".format(\n",
    "                datetime.datetime.utcnow() - train_begin, epoch,\n",
    "                    iteration, running_loss, validation_loss, accuracy,\n",
    "                    optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # save checkpoints and final model\n",
    "    # ------------------------------------------------------------------------\n",
    "    if epoch % args.save_step == 0:\n",
    "        torch.save({\n",
    "            'net': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': net.args\n",
    "        }, os.path.join(args.save_path, 'model_epoch_{}.pth'.format(epoch)))\n",
    "\n",
    "torch.save({\n",
    "    'net': net.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'model_args': net.args\n",
    "}, os.path.join(args.save_path, 'model_final.pth'))\n",
    "\n",
    "np.save(os.path.join(args.save_path, 'log_loss'), log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_np=test_labels.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_np[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.0   recall:  0.0   F1:  0.0   accuracy:  1.0\n",
      "Training Confusion matrix\n",
      "[[50]]\n",
      "Training AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,0],y_pred[:,0])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.0   recall:  0.0   F1:  0.0   accuracy:  1.0\n",
      "Training Confusion matrix\n",
      "[[50]]\n",
      "Training AUC: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/ranking.py:651: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,1],y_pred[:,1])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.8   recall:  0.6153846153846154   F1:  0.6956521739130435   accuracy:  0.86\n",
      "Training Confusion matrix\n",
      "[[35  2]\n",
      " [ 5  8]]\n",
      "Training AUC: 0.7806652806652806\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,2],y_pred[:,2])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.0   recall:  0.0   F1:  0.0   accuracy:  0.96\n",
      "Training Confusion matrix\n",
      "[[48  0]\n",
      " [ 2  0]]\n",
      "Training AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,3],y_pred[:,3])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.7272727272727273   recall:  0.7272727272727273   F1:  0.7272727272727273   accuracy:  0.88\n",
      "Training Confusion matrix\n",
      "[[36  3]\n",
      " [ 3  8]]\n",
      "Training AUC: 0.8251748251748253\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,4],y_pred[:,4])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.0   recall:  0.0   F1:  0.0   accuracy:  0.96\n",
      "Training Confusion matrix\n",
      "[[48  0]\n",
      " [ 2  0]]\n",
      "Training AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,5],y_pred[:,5])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.25   recall:  0.3333333333333333   F1:  0.28571428571428575   accuracy:  0.9\n",
      "Training Confusion matrix\n",
      "[[44  3]\n",
      " [ 2  1]]\n",
      "Training AUC: 0.6347517730496454\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,6],y_pred[:,6])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.0   recall:  0.0   F1:  0.0   accuracy:  0.82\n",
      "Training Confusion matrix\n",
      "[[41  7]\n",
      " [ 2  0]]\n",
      "Training AUC: 0.4270833333333333\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,7],y_pred[:,7])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training precision:  0.0   recall:  0.0   F1:  0.0   accuracy:  0.96\n",
      "Training Confusion matrix\n",
      "[[48  0]\n",
      " [ 2  0]]\n",
      "Training AUC: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/envs/pytorch/lib/python3.5/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(test_labels_np[:,8],y_pred[:,8])\n",
    "print('Training precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('Training Confusion matrix')\n",
    "print(confusion)\n",
    "print('Training AUC:',roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
