{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "\n",
    "import scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data is being read ....\n",
      "(2572, 98)\n",
      "(2572, 109)\n",
      "Training Data has been read and feature engineering is being performed....\n",
      "(2572, 110)\n",
      "['training_data.csv', 'test_data_predictions.csv', 'test_data.csv']\n",
      "0    1344\n",
      "1    1228\n",
      "Name: Stable_compunds, dtype: int64\n",
      "Pearson Correlation has been calculated to build the model in the most relevant features ....\n",
      "Number of Results to train on: (2572,)\n",
      "Number of Training Features before Pearson correlation: 98\n",
      "(99, 99)\n",
      "Pearson Correlation has identified 38 with  0.1\n",
      "Number of Training Features after Pearson correlation: 38\n",
      "(2572, 38)\n",
      "(2572, 38)\n",
      "(2572, 38)\n",
      "(2572, 38)\n",
      "(2572, 20)\n",
      "(2572, 20)\n",
      "Pearson Correlation in PCA Space has identified 5 with  0.05\n",
      "Number of Training Features after Pearson correlation in PCA Space: 5\n",
      "Training Model Using Z-normalized Data\n",
      "(2314, 38) (2314,)\n",
      "(258, 38) (258,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import scores\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "\n",
    "# # Reading in the Data\n",
    "\n",
    "path_f=os.getcwd()\n",
    "\n",
    "path_f_1=os.path.join(path_f, 'data')\n",
    "\n",
    "\n",
    "names=[]\n",
    "for files_txts in os.listdir(path_f_1):\n",
    "    if files_txts.endswith(\".csv\"):\n",
    "        #print(files_txts)\n",
    "        names.append(files_txts)\n",
    "        \n",
    "path_train=os.path.join(path_f_1, names[0])\n",
    "path_test=os.path.join(path_f_1, names[1])\n",
    "\n",
    "df_train=pd.read_csv(path_train)\n",
    "df_train.shape\n",
    "\n",
    "\n",
    "# ## Data Manipulation\n",
    "print('Training Data is being read ....')\n",
    "#  - Transforming the outcome to a numpy vector\n",
    "\n",
    "stab_vector=df_train['stabilityVec'].values\n",
    "y=[]\n",
    "for x in stab_vector:\n",
    "    #print(x)\n",
    "    a=np.fromstring(x[1:-1],sep=',').astype(int)\n",
    "    y.append(a)\n",
    "y=np.array(y) \n",
    "\n",
    "df_tmp = pd.DataFrame(y, columns = ['A', 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B','B'])\n",
    "stab_vec_list=[ 'A91B', 'A82B','A73B','A64B','A55B','A46B','A37B','A28B','A19B']\n",
    "\n",
    "df_train=df_train.drop(\"stabilityVec\",axis=1) #removing the results which originally are a string\n",
    "feature_cols=list(df_train)\n",
    "\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train['formulaA']=df_train['formulaA_elements_Number']\n",
    "df_train['formulaB']=df_train['formulaB_elements_Number']\n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "# ### Input Data Normalization and Feature Engineering\n",
    "print('Training Data has been read and feature engineering is being performed....')\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "df_tmp_stable = pd.DataFrame( columns = ['Stable_compunds'])\n",
    "df_tmp_stable['Stable_compunds']=np.logical_not(y_all.sum(axis=1)==0).astype(int) ## A one means it has a stable value  a 0 \n",
    "\n",
    "df_train=pd.concat([df_train, df_tmp_stable],axis=1)\n",
    "print(df_train.shape)\n",
    "\n",
    "df_train.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "print(names)\n",
    "\n",
    "\n",
    "# ## Selecting Output for Component 1 of Stability Vector\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "## Observing how many element pairs produce a stable compound per % and overall\n",
    "\n",
    "y_all=df_train[stab_vec_list]\n",
    "\n",
    "\n",
    "    \n",
    "y = df_train['Stable_compunds']\n",
    "print(y.value_counts())\n",
    "\n",
    "\n",
    "\n",
    "# ## Pearson Correlation and Input Normalization\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "# Pearson Correlation to Identify the features that influence the most on the output \n",
    "print('Pearson Correlation has been calculated to build the model in the most relevant features ....')\n",
    "X_train_new_all=df_train[feature_cols] #This means we will only train on the elements that create a stable compound for this component of the stability vector and have at least one stable compound\n",
    "\n",
    "y_new=y \n",
    "print('Number of Results to train on:',y_new.shape)\n",
    "print('Number of Training Features before Pearson correlation:', X_train_new_all.shape[1])\n",
    "\n",
    "corr_df=pd.concat([X_train_new_all, y_new],axis=1)\n",
    "a=corr_df.corr()\n",
    "#a['Stable_compunds'].hist(bins=7, figsize=(18, 12), xlabelsize=10)\n",
    "\n",
    "## Incorporating the Features that contribute the most based on a pearson correlation coefficient threshold\n",
    "\n",
    "thr=.1\n",
    "\n",
    "#a['Stable_compunds'].hist(bins=7, figsize=(18, 12), xlabelsize=10)\n",
    "\n",
    "print(a.shape)\n",
    "\n",
    "\n",
    "corr_variables=list(a[a['Stable_compunds'].abs()>thr].index)\n",
    "\n",
    "del(corr_variables[-1])\n",
    "\n",
    "\n",
    "print('Pearson Correlation has identified', len(corr_variables), 'with ', str(thr) )\n",
    "## Normalization of Input Data\n",
    "\n",
    "## Using Un-normalized data as input\n",
    "X_train_new=df_train[corr_variables]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation:', X_train_new.shape[1])\n",
    "\n",
    "\n",
    "# Normalizing such that the magnitude is one\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X_train_new_mag_1=normalize(X_train_new, axis=1) # vector magnitude is one\n",
    "print(X_train_new_mag_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing by Zscore\n",
    "from scipy.stats import zscore\n",
    "X_train_new_Z_score=X_train_new.apply(zscore)\n",
    "print(X_train_new_Z_score.shape)\n",
    "\n",
    "\n",
    "\n",
    "## Normalizing so that range is 0-1\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_new_0_1=min_max_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_0_1.shape)\n",
    "\n",
    "\n",
    "## Normalizing so that range is -1 to 1\n",
    "from sklearn import preprocessing\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "X_train_new_m1_p1=max_abs_scaler.fit_transform(X_train_new)\n",
    "print(X_train_new_m1_p1.shape)\n",
    "\n",
    "\n",
    "# Using PCA as input\n",
    "X_train_4_PCA=df_train[feature_cols]\n",
    "indx_4_PC=X_train_4_PCA.index\n",
    "X_train_new_mag_1_PCA=normalize(X_train_4_PCA, axis=1)\n",
    "\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_new_mag_1_PCA)\n",
    "components = pca.components_[:20,:]\n",
    "new_data = np.dot(X_train_new_mag_1_PCA, components.T)\n",
    "X_train_new_PCA=new_data\n",
    "\n",
    "print(X_train_new_PCA.shape)\n",
    "\n",
    "## Using Pearson Correlation in PCA\n",
    "df1= pd.DataFrame(data=X_train_new_PCA, index=indx_4_PC)\n",
    "print(df1.shape)\n",
    "\n",
    "corr_df_PCA=pd.concat([df1, y_new],axis=1)\n",
    "\n",
    "\n",
    "a_PCA=corr_df_PCA.corr()\n",
    "\n",
    "thr=.05\n",
    "corr_variables_PCA=list(a_PCA[a_PCA['Stable_compunds'].abs()>thr].index)\n",
    "\n",
    "\n",
    "del(corr_variables_PCA[-1])\n",
    "\n",
    "print('Pearson Correlation in PCA Space has identified', len(corr_variables_PCA), 'with ', str(thr) )\n",
    "\n",
    "X_train_PCA_PC=df1[corr_variables_PCA]\n",
    "\n",
    "print('Number of Training Features after Pearson correlation in PCA Space:', X_train_PCA_PC.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Model Generation\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "print('Training Model Using Z-normalized Data')\n",
    "## test-train split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_new_Z_score, y_new,\n",
    "                                                    test_size=.15,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=42)\n",
    "\n",
    "print(X_train.shape,y_train.shape)\n",
    "print(X_test.shape,y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- Optimal Random Forest --\n",
      "Optimal precision:  0.8484848484848485   recall:  0.9491525423728814   F1:  0.896   accuracy:  0.8992248062015504\n",
      "optimal Confusion matrix\n",
      "[[120  20]\n",
      " [  6 112]]\n",
      "Optimal AUC: 0.9031476997578695\n",
      " -- Optimal Decision Tree --\n",
      "Optimal precision:  0.8449612403100775   recall:  0.923728813559322   F1:  0.8825910931174089   accuracy:  0.8875968992248062\n",
      "optimal Confusion matrix\n",
      "[[120  20]\n",
      " [  9 109]]\n",
      "Optimal AUC: 0.8904358353510896\n",
      " -- Optimal KNN --\n",
      "Optimal precision:  0.8571428571428571   recall:  0.9152542372881356   F1:  0.8852459016393444   accuracy:  0.8914728682170543\n",
      "optimal Confusion matrix\n",
      "[[122  18]\n",
      " [ 10 108]]\n",
      "Optimal AUC: 0.8933414043583535\n",
      " -- Optimal SVM --\n",
      "Optimal precision:  0.8461538461538461   recall:  0.9322033898305084   F1:  0.8870967741935484   accuracy:  0.8914728682170543\n",
      "optimal Confusion matrix\n",
      "[[120  20]\n",
      " [  8 110]]\n",
      "Optimal AUC: 0.8946731234866829\n",
      "-------- Bagging and Boosting Result-------\n",
      "------- ADA Boosting Random Forest Classifier-------\n",
      "Optimal precision:  0.8661417322834646   recall:  0.9322033898305084   F1:  0.8979591836734694   accuracy:  0.9031007751937985\n",
      "optimal Confusion matrix\n",
      "[[123  17]\n",
      " [  8 110]]\n",
      "Optimal AUC: 0.9053874092009685\n",
      "------- ADA Boosting Decision Tree Classifier-------\n",
      "Optimal precision:  0.8473282442748091   recall:  0.940677966101695   F1:  0.891566265060241   accuracy:  0.8953488372093024\n",
      "optimal Confusion matrix\n",
      "[[120  20]\n",
      " [  7 111]]\n",
      "Optimal AUC: 0.8989104116222761\n",
      "------- Gradient Boosting Classifier-------\n",
      "Optimal precision:  0.8473282442748091   recall:  0.940677966101695   F1:  0.891566265060241   accuracy:  0.8953488372093024\n",
      "optimal Confusion matrix\n",
      "[[120  20]\n",
      " [  7 111]]\n",
      "Optimal AUC: 0.8989104116222761\n",
      "------- Extra Trees Classifier-------\n",
      "Optimal precision:  0.7872340425531915   recall:  0.940677966101695   F1:  0.8571428571428572   accuracy:  0.8565891472868217\n",
      "optimal Confusion matrix\n",
      "[[110  30]\n",
      " [  7 111]]\n",
      "Optimal AUC: 0.8631961259079903\n"
     ]
    }
   ],
   "source": [
    "## Fitting best Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import sklearn.tree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "print(' -- Optimal Random Forest --')\n",
    "\n",
    "\n",
    "rfc_opt = RandomForestClassifier(n_estimators=5,\n",
    "                                 criterion='entropy',\n",
    "                                 bootstrap=False,\n",
    "                                 max_depth=10, \n",
    "                                 class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                 min_samples_split=10,\n",
    "                                 min_samples_leaf=1,\n",
    "                                 min_impurity_decrease=5e-7,\n",
    "                                 random_state=0,\n",
    "                                 n_jobs=-1)\n",
    "rfc_opt.fit(X_train, y_train)\n",
    "y_pred = rfc_opt.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "## Fitting best Model\n",
    "print(' -- Optimal Decision Tree --')\n",
    "## Fitting best Model\n",
    "#[cr,max_d,sp,min_sample_sp,min_samples_le,min_impurity_sp]\n",
    "rfc_opt_DT = sklearn.tree.DecisionTreeClassifier(class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                                 criterion='entropy',\n",
    "                                                 max_depth=100,\n",
    "                                                 random_state=0, \n",
    "                                                 splitter='best',\n",
    "                                                 min_samples_split=2,\n",
    "                                                 min_samples_leaf=1,\n",
    "                                                 min_impurity_decrease=5e-7)\n",
    "\n",
    "\n",
    "rfc_opt_DT.fit(X_train, y_train)\n",
    "y_pred = rfc_opt_DT.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print(' -- Optimal KNN --')\n",
    "rf_opt_KNN=KNeighborsClassifier(algorithm='auto',\n",
    "                                metric='minkowski',\n",
    "                                n_jobs=-1, \n",
    "                                n_neighbors=1,\n",
    "                                p=1,\n",
    "                                weights='distance')\n",
    "\n",
    "rf_opt_KNN.fit(X_train, y_train)\n",
    "y_pred = rf_opt_KNN.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print(' -- Optimal SVM --')\n",
    "\n",
    "\n",
    "rfc_opt_SVM = sklearn.svm.SVC(kernel='rbf', \n",
    "                      gamma=.1,C=5,\n",
    "                      random_state=0,\n",
    "                      class_weight={0:y_train.mean(), 1:1-y_train.mean()})\n",
    "\n",
    "rfc_opt_SVM.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_SVM.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "\n",
    "print('-------- Bagging and Boosting Result-------')\n",
    "\n",
    "print('------- ADA Boosting Random Forest Classifier-------')\n",
    "\n",
    "#[estimator,cr,boots,max_d,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "\n",
    "rfc_opT_rf =RandomForestClassifier(n_estimators=10,\n",
    "                                 criterion='entropy',\n",
    "                                 bootstrap=True,\n",
    "                                 max_depth=10, \n",
    "                                 class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                 min_samples_split=2,\n",
    "                                 min_samples_leaf=5,\n",
    "                                 min_impurity_decrease=5e-7,\n",
    "                                 random_state=0,\n",
    "                                 n_jobs=-1)\n",
    "clf_RF = AdaBoostClassifier(base_estimator=rfc_opT_rf,\n",
    "                         n_estimators=100,\n",
    "                         learning_rate=.1)\n",
    "\n",
    "clf_RF.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf_RF.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- ADA Boosting Decision Tree Classifier-------')\n",
    "\n",
    "#([cr,max_d,sp,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "\n",
    "rfc = sklearn.tree.DecisionTreeClassifier(class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                                 criterion='entropy',\n",
    "                                                 max_depth=5,\n",
    "                                                 random_state=0, \n",
    "                                                 splitter='random',\n",
    "                                                 min_samples_split=6,\n",
    "                                                 min_samples_leaf=1,\n",
    "                                                 min_impurity_decrease=5e-7)\n",
    "\n",
    "clf = AdaBoostClassifier(base_estimator=rfc,\n",
    "                         n_estimators=1000,\n",
    "                         learning_rate=.1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- Gradient Boosting Classifier-------')\n",
    "\n",
    "#[max_d,min_sample_sp,min_samples_le,min_impurity_sp,num_e,lr]\n",
    "rfc_opt_GRAD=GradientBoostingClassifier(n_estimators=700,\n",
    "                                         learning_rate=.01,\n",
    "                                         min_samples_split=6,\n",
    "                                         min_samples_leaf=1,\n",
    "                                         max_depth=5,\n",
    "                                         random_state=0)\n",
    "\n",
    "rfc_opt_GRAD.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_GRAD.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n",
    "\n",
    "print('------- Extra Trees Classifier-------')\n",
    "\n",
    "#[estimator,cr,boots,max_d,min_sample_sp,min_samples_le,min_impurity_sp]\n",
    "\n",
    "rfc_opt_Extra= ExtraTreesClassifier(n_estimators=5,\n",
    "                                     criterion='entropy',\n",
    "                                     bootstrap=False,\n",
    "                                     max_depth=10,\n",
    "                                     class_weight={0:y_train.mean(), 1:1-y_train.mean()},\n",
    "                                     min_samples_split=10,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_impurity_decrease=5e-7,\n",
    "                                     random_state=0,n_jobs=-1)\n",
    "\n",
    "rfc_opt_Extra.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rfc_opt_Extra.predict(X_test)\n",
    "\n",
    "precision,recall,F1,accuracy,confusion,roc_auc=scores.scores(y_test,y_pred)\n",
    "print('Optimal precision: ', precision, '  recall: ', recall, '  F1: ', F1, '  accuracy: ', accuracy)\n",
    "print('optimal Confusion matrix')\n",
    "print(confusion)\n",
    "print('Optimal AUC:',roc_auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Best Model\n",
    "\n",
    "Selecting the model that has the highest AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Best_model_at_least_one_element.sav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "filename = 'Best_model_at_least_one_element.sav'\n",
    "model=rfc_opt\n",
    "joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corr_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('var_list_stable.var', 'wb') as f:\n",
    "    pickle.dump(corr_variables, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
